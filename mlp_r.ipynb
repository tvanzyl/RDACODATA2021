{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"neuralnet\")\n",
    "# install.packages(\"ggplot2\")\n",
    "# install.packages(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n"
     ]
    }
   ],
   "source": [
    "# library(dplyr)\n",
    "library(neuralnet)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(tibble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(neuralnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata <- read.csv(\"breast_cancer_data.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets clean the data a little\n",
    "data <- rawdata[,-c(0:1)]\n",
    "data$diagnosis <- as.factor(data$diagnosis)\n",
    "# the 32 column is not right\n",
    "data[,32] <- NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 31</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>diagnosis</th><th scope=col>radius_mean</th><th scope=col>texture_mean</th><th scope=col>perimeter_mean</th><th scope=col>area_mean</th><th scope=col>smoothness_mean</th><th scope=col>compactness_mean</th><th scope=col>concavity_mean</th><th scope=col>concave.points_mean</th><th scope=col>symmetry_mean</th><th scope=col>⋯</th><th scope=col>radius_worst</th><th scope=col>texture_worst</th><th scope=col>perimeter_worst</th><th scope=col>area_worst</th><th scope=col>smoothness_worst</th><th scope=col>compactness_worst</th><th scope=col>concavity_worst</th><th scope=col>concave.points_worst</th><th scope=col>symmetry_worst</th><th scope=col>fractal_dimension_worst</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>M</td><td>17.99</td><td>10.38</td><td>122.80</td><td>1001.0</td><td>0.11840</td><td>0.27760</td><td>0.3001</td><td>0.14710</td><td>0.2419</td><td>⋯</td><td>25.38</td><td>17.33</td><td>184.60</td><td>2019.0</td><td>0.1622</td><td>0.6656</td><td>0.7119</td><td>0.2654</td><td>0.4601</td><td>0.11890</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>M</td><td>20.57</td><td>17.77</td><td>132.90</td><td>1326.0</td><td>0.08474</td><td>0.07864</td><td>0.0869</td><td>0.07017</td><td>0.1812</td><td>⋯</td><td>24.99</td><td>23.41</td><td>158.80</td><td>1956.0</td><td>0.1238</td><td>0.1866</td><td>0.2416</td><td>0.1860</td><td>0.2750</td><td>0.08902</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>M</td><td>19.69</td><td>21.25</td><td>130.00</td><td>1203.0</td><td>0.10960</td><td>0.15990</td><td>0.1974</td><td>0.12790</td><td>0.2069</td><td>⋯</td><td>23.57</td><td>25.53</td><td>152.50</td><td>1709.0</td><td>0.1444</td><td>0.4245</td><td>0.4504</td><td>0.2430</td><td>0.3613</td><td>0.08758</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>M</td><td>11.42</td><td>20.38</td><td> 77.58</td><td> 386.1</td><td>0.14250</td><td>0.28390</td><td>0.2414</td><td>0.10520</td><td>0.2597</td><td>⋯</td><td>14.91</td><td>26.50</td><td> 98.87</td><td> 567.7</td><td>0.2098</td><td>0.8663</td><td>0.6869</td><td>0.2575</td><td>0.6638</td><td>0.17300</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>M</td><td>20.29</td><td>14.34</td><td>135.10</td><td>1297.0</td><td>0.10030</td><td>0.13280</td><td>0.1980</td><td>0.10430</td><td>0.1809</td><td>⋯</td><td>22.54</td><td>16.67</td><td>152.20</td><td>1575.0</td><td>0.1374</td><td>0.2050</td><td>0.4000</td><td>0.1625</td><td>0.2364</td><td>0.07678</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>M</td><td>12.45</td><td>15.70</td><td> 82.57</td><td> 477.1</td><td>0.12780</td><td>0.17000</td><td>0.1578</td><td>0.08089</td><td>0.2087</td><td>⋯</td><td>15.47</td><td>23.75</td><td>103.40</td><td> 741.6</td><td>0.1791</td><td>0.5249</td><td>0.5355</td><td>0.1741</td><td>0.3985</td><td>0.12440</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 31\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & compactness\\_mean & concavity\\_mean & concave.points\\_mean & symmetry\\_mean & ⋯ & radius\\_worst & texture\\_worst & perimeter\\_worst & area\\_worst & smoothness\\_worst & compactness\\_worst & concavity\\_worst & concave.points\\_worst & symmetry\\_worst & fractal\\_dimension\\_worst\\\\\n",
       "  & <fct> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & M & 17.99 & 10.38 & 122.80 & 1001.0 & 0.11840 & 0.27760 & 0.3001 & 0.14710 & 0.2419 & ⋯ & 25.38 & 17.33 & 184.60 & 2019.0 & 0.1622 & 0.6656 & 0.7119 & 0.2654 & 0.4601 & 0.11890\\\\\n",
       "\t2 & M & 20.57 & 17.77 & 132.90 & 1326.0 & 0.08474 & 0.07864 & 0.0869 & 0.07017 & 0.1812 & ⋯ & 24.99 & 23.41 & 158.80 & 1956.0 & 0.1238 & 0.1866 & 0.2416 & 0.1860 & 0.2750 & 0.08902\\\\\n",
       "\t3 & M & 19.69 & 21.25 & 130.00 & 1203.0 & 0.10960 & 0.15990 & 0.1974 & 0.12790 & 0.2069 & ⋯ & 23.57 & 25.53 & 152.50 & 1709.0 & 0.1444 & 0.4245 & 0.4504 & 0.2430 & 0.3613 & 0.08758\\\\\n",
       "\t4 & M & 11.42 & 20.38 &  77.58 &  386.1 & 0.14250 & 0.28390 & 0.2414 & 0.10520 & 0.2597 & ⋯ & 14.91 & 26.50 &  98.87 &  567.7 & 0.2098 & 0.8663 & 0.6869 & 0.2575 & 0.6638 & 0.17300\\\\\n",
       "\t5 & M & 20.29 & 14.34 & 135.10 & 1297.0 & 0.10030 & 0.13280 & 0.1980 & 0.10430 & 0.1809 & ⋯ & 22.54 & 16.67 & 152.20 & 1575.0 & 0.1374 & 0.2050 & 0.4000 & 0.1625 & 0.2364 & 0.07678\\\\\n",
       "\t6 & M & 12.45 & 15.70 &  82.57 &  477.1 & 0.12780 & 0.17000 & 0.1578 & 0.08089 & 0.2087 & ⋯ & 15.47 & 23.75 & 103.40 &  741.6 & 0.1791 & 0.5249 & 0.5355 & 0.1741 & 0.3985 & 0.12440\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 31\n",
       "\n",
       "| <!--/--> | diagnosis &lt;fct&gt; | radius_mean &lt;dbl&gt; | texture_mean &lt;dbl&gt; | perimeter_mean &lt;dbl&gt; | area_mean &lt;dbl&gt; | smoothness_mean &lt;dbl&gt; | compactness_mean &lt;dbl&gt; | concavity_mean &lt;dbl&gt; | concave.points_mean &lt;dbl&gt; | symmetry_mean &lt;dbl&gt; | ⋯ ⋯ | radius_worst &lt;dbl&gt; | texture_worst &lt;dbl&gt; | perimeter_worst &lt;dbl&gt; | area_worst &lt;dbl&gt; | smoothness_worst &lt;dbl&gt; | compactness_worst &lt;dbl&gt; | concavity_worst &lt;dbl&gt; | concave.points_worst &lt;dbl&gt; | symmetry_worst &lt;dbl&gt; | fractal_dimension_worst &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | M | 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | ⋯ | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 |\n",
       "| 2 | M | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | ⋯ | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 |\n",
       "| 3 | M | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | ⋯ | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 |\n",
       "| 4 | M | 11.42 | 20.38 |  77.58 |  386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | ⋯ | 14.91 | 26.50 |  98.87 |  567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 |\n",
       "| 5 | M | 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | ⋯ | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 |\n",
       "| 6 | M | 12.45 | 15.70 |  82.57 |  477.1 | 0.12780 | 0.17000 | 0.1578 | 0.08089 | 0.2087 | ⋯ | 15.47 | 23.75 | 103.40 |  741.6 | 0.1791 | 0.5249 | 0.5355 | 0.1741 | 0.3985 | 0.12440 |\n",
       "\n"
      ],
      "text/plain": [
       "  diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean\n",
       "1 M         17.99       10.38        122.80         1001.0    0.11840        \n",
       "2 M         20.57       17.77        132.90         1326.0    0.08474        \n",
       "3 M         19.69       21.25        130.00         1203.0    0.10960        \n",
       "4 M         11.42       20.38         77.58          386.1    0.14250        \n",
       "5 M         20.29       14.34        135.10         1297.0    0.10030        \n",
       "6 M         12.45       15.70         82.57          477.1    0.12780        \n",
       "  compactness_mean concavity_mean concave.points_mean symmetry_mean ⋯\n",
       "1 0.27760          0.3001         0.14710             0.2419        ⋯\n",
       "2 0.07864          0.0869         0.07017             0.1812        ⋯\n",
       "3 0.15990          0.1974         0.12790             0.2069        ⋯\n",
       "4 0.28390          0.2414         0.10520             0.2597        ⋯\n",
       "5 0.13280          0.1980         0.10430             0.1809        ⋯\n",
       "6 0.17000          0.1578         0.08089             0.2087        ⋯\n",
       "  radius_worst texture_worst perimeter_worst area_worst smoothness_worst\n",
       "1 25.38        17.33         184.60          2019.0     0.1622          \n",
       "2 24.99        23.41         158.80          1956.0     0.1238          \n",
       "3 23.57        25.53         152.50          1709.0     0.1444          \n",
       "4 14.91        26.50          98.87           567.7     0.2098          \n",
       "5 22.54        16.67         152.20          1575.0     0.1374          \n",
       "6 15.47        23.75         103.40           741.6     0.1791          \n",
       "  compactness_worst concavity_worst concave.points_worst symmetry_worst\n",
       "1 0.6656            0.7119          0.2654               0.4601        \n",
       "2 0.1866            0.2416          0.1860               0.2750        \n",
       "3 0.4245            0.4504          0.2430               0.3613        \n",
       "4 0.8663            0.6869          0.2575               0.6638        \n",
       "5 0.2050            0.4000          0.1625               0.2364        \n",
       "6 0.5249            0.5355          0.1741               0.3985        \n",
       "  fractal_dimension_worst\n",
       "1 0.11890                \n",
       "2 0.08902                \n",
       "3 0.08758                \n",
       "4 0.17300                \n",
       "5 0.07678                \n",
       "6 0.12440                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "data_index <- createDataPartition(data$diagnosis, p=0.7, list=FALSE)\n",
    "train_data_X <- data[data_index, -1]\n",
    "train_data_y <- data[data_index, 1]=='M'\n",
    "test_data_X <- data[-data_index, -1]\n",
    "test_data_y <-data[-data_index, 1]=='M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 399 × 31</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>train_data_y</th><th scope=col>radius_mean</th><th scope=col>texture_mean</th><th scope=col>perimeter_mean</th><th scope=col>area_mean</th><th scope=col>smoothness_mean</th><th scope=col>compactness_mean</th><th scope=col>concavity_mean</th><th scope=col>concave.points_mean</th><th scope=col>symmetry_mean</th><th scope=col>⋯</th><th scope=col>radius_worst</th><th scope=col>texture_worst</th><th scope=col>perimeter_worst</th><th scope=col>area_worst</th><th scope=col>smoothness_worst</th><th scope=col>compactness_worst</th><th scope=col>concavity_worst</th><th scope=col>concave.points_worst</th><th scope=col>symmetry_worst</th><th scope=col>fractal_dimension_worst</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td> TRUE</td><td>20.57</td><td>17.77</td><td>132.90</td><td>1326.0</td><td>0.08474</td><td>0.07864</td><td>0.08690</td><td>0.07017</td><td>0.1812</td><td>⋯</td><td>24.99</td><td>23.41</td><td>158.80</td><td>1956.0</td><td>0.12380</td><td>0.18660</td><td>0.24160</td><td>0.18600</td><td>0.2750</td><td>0.08902</td></tr>\n",
       "\t<tr><th scope=row>3</th><td> TRUE</td><td>19.69</td><td>21.25</td><td>130.00</td><td>1203.0</td><td>0.10960</td><td>0.15990</td><td>0.19740</td><td>0.12790</td><td>0.2069</td><td>⋯</td><td>23.57</td><td>25.53</td><td>152.50</td><td>1709.0</td><td>0.14440</td><td>0.42450</td><td>0.45040</td><td>0.24300</td><td>0.3613</td><td>0.08758</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> TRUE</td><td>11.42</td><td>20.38</td><td> 77.58</td><td> 386.1</td><td>0.14250</td><td>0.28390</td><td>0.24140</td><td>0.10520</td><td>0.2597</td><td>⋯</td><td>14.91</td><td>26.50</td><td> 98.87</td><td> 567.7</td><td>0.20980</td><td>0.86630</td><td>0.68690</td><td>0.25750</td><td>0.6638</td><td>0.17300</td></tr>\n",
       "\t<tr><th scope=row>10</th><td> TRUE</td><td>12.46</td><td>24.04</td><td> 83.97</td><td> 475.9</td><td>0.11860</td><td>0.23960</td><td>0.22730</td><td>0.08543</td><td>0.2030</td><td>⋯</td><td>15.09</td><td>40.68</td><td> 97.65</td><td> 711.4</td><td>0.18530</td><td>1.05800</td><td>1.10500</td><td>0.22100</td><td>0.4366</td><td>0.20750</td></tr>\n",
       "\t<tr><th scope=row>11</th><td> TRUE</td><td>16.02</td><td>23.24</td><td>102.70</td><td> 797.8</td><td>0.08206</td><td>0.06669</td><td>0.03299</td><td>0.03323</td><td>0.1528</td><td>⋯</td><td>19.19</td><td>33.88</td><td>123.80</td><td>1150.0</td><td>0.11810</td><td>0.15510</td><td>0.14590</td><td>0.09975</td><td>0.2948</td><td>0.08452</td></tr>\n",
       "\t<tr><th scope=row>13</th><td> TRUE</td><td>19.17</td><td>24.80</td><td>132.40</td><td>1123.0</td><td>0.09740</td><td>0.24580</td><td>0.20650</td><td>0.11180</td><td>0.2397</td><td>⋯</td><td>20.96</td><td>29.94</td><td>151.70</td><td>1332.0</td><td>0.10370</td><td>0.39030</td><td>0.36390</td><td>0.17670</td><td>0.3176</td><td>0.10230</td></tr>\n",
       "\t<tr><th scope=row>14</th><td> TRUE</td><td>15.85</td><td>23.95</td><td>103.70</td><td> 782.7</td><td>0.08401</td><td>0.10020</td><td>0.09938</td><td>0.05364</td><td>0.1847</td><td>⋯</td><td>16.84</td><td>27.66</td><td>112.00</td><td> 876.5</td><td>0.11310</td><td>0.19240</td><td>0.23220</td><td>0.11190</td><td>0.2809</td><td>0.06287</td></tr>\n",
       "\t<tr><th scope=row>15</th><td> TRUE</td><td>13.73</td><td>22.61</td><td> 93.60</td><td> 578.3</td><td>0.11310</td><td>0.22930</td><td>0.21280</td><td>0.08025</td><td>0.2069</td><td>⋯</td><td>15.03</td><td>32.01</td><td>108.80</td><td> 697.7</td><td>0.16510</td><td>0.77250</td><td>0.69430</td><td>0.22080</td><td>0.3596</td><td>0.14310</td></tr>\n",
       "\t<tr><th scope=row>16</th><td> TRUE</td><td>14.54</td><td>27.54</td><td> 96.73</td><td> 658.8</td><td>0.11390</td><td>0.15950</td><td>0.16390</td><td>0.07364</td><td>0.2303</td><td>⋯</td><td>17.46</td><td>37.13</td><td>124.10</td><td> 943.2</td><td>0.16780</td><td>0.65770</td><td>0.70260</td><td>0.17120</td><td>0.4218</td><td>0.13410</td></tr>\n",
       "\t<tr><th scope=row>17</th><td> TRUE</td><td>14.68</td><td>20.13</td><td> 94.74</td><td> 684.5</td><td>0.09867</td><td>0.07200</td><td>0.07395</td><td>0.05259</td><td>0.1586</td><td>⋯</td><td>19.07</td><td>30.88</td><td>123.40</td><td>1138.0</td><td>0.14640</td><td>0.18710</td><td>0.29140</td><td>0.16090</td><td>0.3029</td><td>0.08216</td></tr>\n",
       "\t<tr><th scope=row>18</th><td> TRUE</td><td>16.13</td><td>20.68</td><td>108.10</td><td> 798.8</td><td>0.11700</td><td>0.20220</td><td>0.17220</td><td>0.10280</td><td>0.2164</td><td>⋯</td><td>20.96</td><td>31.48</td><td>136.80</td><td>1315.0</td><td>0.17890</td><td>0.42330</td><td>0.47840</td><td>0.20730</td><td>0.3706</td><td>0.11420</td></tr>\n",
       "\t<tr><th scope=row>19</th><td> TRUE</td><td>19.81</td><td>22.15</td><td>130.00</td><td>1260.0</td><td>0.09831</td><td>0.10270</td><td>0.14790</td><td>0.09498</td><td>0.1582</td><td>⋯</td><td>27.32</td><td>30.88</td><td>186.80</td><td>2398.0</td><td>0.15120</td><td>0.31500</td><td>0.53720</td><td>0.23880</td><td>0.2768</td><td>0.07615</td></tr>\n",
       "\t<tr><th scope=row>23</th><td> TRUE</td><td>15.34</td><td>14.26</td><td>102.50</td><td> 704.4</td><td>0.10730</td><td>0.21350</td><td>0.20770</td><td>0.09756</td><td>0.2521</td><td>⋯</td><td>18.07</td><td>19.08</td><td>125.10</td><td> 980.9</td><td>0.13900</td><td>0.59540</td><td>0.63050</td><td>0.23930</td><td>0.4667</td><td>0.09946</td></tr>\n",
       "\t<tr><th scope=row>24</th><td> TRUE</td><td>21.16</td><td>23.04</td><td>137.20</td><td>1404.0</td><td>0.09428</td><td>0.10220</td><td>0.10970</td><td>0.08632</td><td>0.1769</td><td>⋯</td><td>29.17</td><td>35.59</td><td>188.00</td><td>2615.0</td><td>0.14010</td><td>0.26000</td><td>0.31550</td><td>0.20090</td><td>0.2822</td><td>0.07526</td></tr>\n",
       "\t<tr><th scope=row>27</th><td> TRUE</td><td>14.58</td><td>21.53</td><td> 97.41</td><td> 644.8</td><td>0.10540</td><td>0.18680</td><td>0.14250</td><td>0.08783</td><td>0.2252</td><td>⋯</td><td>17.62</td><td>33.21</td><td>122.40</td><td> 896.9</td><td>0.15250</td><td>0.66430</td><td>0.55390</td><td>0.27010</td><td>0.4264</td><td>0.12750</td></tr>\n",
       "\t<tr><th scope=row>28</th><td> TRUE</td><td>18.61</td><td>20.25</td><td>122.10</td><td>1094.0</td><td>0.09440</td><td>0.10660</td><td>0.14900</td><td>0.07731</td><td>0.1697</td><td>⋯</td><td>21.31</td><td>27.26</td><td>139.90</td><td>1403.0</td><td>0.13380</td><td>0.21170</td><td>0.34460</td><td>0.14900</td><td>0.2341</td><td>0.07421</td></tr>\n",
       "\t<tr><th scope=row>29</th><td> TRUE</td><td>15.30</td><td>25.27</td><td>102.40</td><td> 732.4</td><td>0.10820</td><td>0.16970</td><td>0.16830</td><td>0.08751</td><td>0.1926</td><td>⋯</td><td>20.27</td><td>36.71</td><td>149.30</td><td>1269.0</td><td>0.16410</td><td>0.61100</td><td>0.63350</td><td>0.20240</td><td>0.4027</td><td>0.09876</td></tr>\n",
       "\t<tr><th scope=row>32</th><td> TRUE</td><td>11.84</td><td>18.70</td><td> 77.93</td><td> 440.6</td><td>0.11090</td><td>0.15160</td><td>0.12180</td><td>0.05182</td><td>0.2301</td><td>⋯</td><td>16.82</td><td>28.12</td><td>119.40</td><td> 888.7</td><td>0.16370</td><td>0.57750</td><td>0.69560</td><td>0.15460</td><td>0.4761</td><td>0.14020</td></tr>\n",
       "\t<tr><th scope=row>33</th><td> TRUE</td><td>17.02</td><td>23.98</td><td>112.80</td><td> 899.3</td><td>0.11970</td><td>0.14960</td><td>0.24170</td><td>0.12030</td><td>0.2248</td><td>⋯</td><td>20.88</td><td>32.09</td><td>136.10</td><td>1344.0</td><td>0.16340</td><td>0.35590</td><td>0.55880</td><td>0.18470</td><td>0.3530</td><td>0.08482</td></tr>\n",
       "\t<tr><th scope=row>34</th><td> TRUE</td><td>19.27</td><td>26.47</td><td>127.90</td><td>1162.0</td><td>0.09401</td><td>0.17190</td><td>0.16570</td><td>0.07593</td><td>0.1853</td><td>⋯</td><td>24.15</td><td>30.90</td><td>161.40</td><td>1813.0</td><td>0.15090</td><td>0.65900</td><td>0.60910</td><td>0.17850</td><td>0.3672</td><td>0.11230</td></tr>\n",
       "\t<tr><th scope=row>36</th><td> TRUE</td><td>16.74</td><td>21.59</td><td>110.10</td><td> 869.5</td><td>0.09610</td><td>0.13360</td><td>0.13480</td><td>0.06018</td><td>0.1896</td><td>⋯</td><td>20.01</td><td>29.02</td><td>133.50</td><td>1229.0</td><td>0.15630</td><td>0.38350</td><td>0.54090</td><td>0.18130</td><td>0.4863</td><td>0.08633</td></tr>\n",
       "\t<tr><th scope=row>37</th><td> TRUE</td><td>14.25</td><td>21.72</td><td> 93.63</td><td> 633.0</td><td>0.09823</td><td>0.10980</td><td>0.13190</td><td>0.05598</td><td>0.1885</td><td>⋯</td><td>15.89</td><td>30.36</td><td>116.20</td><td> 799.6</td><td>0.14460</td><td>0.42380</td><td>0.51860</td><td>0.14470</td><td>0.3591</td><td>0.10140</td></tr>\n",
       "\t<tr><th scope=row>38</th><td>FALSE</td><td>13.03</td><td>18.42</td><td> 82.61</td><td> 523.8</td><td>0.08983</td><td>0.03766</td><td>0.02562</td><td>0.02923</td><td>0.1467</td><td>⋯</td><td>13.30</td><td>22.81</td><td> 84.46</td><td> 545.9</td><td>0.09701</td><td>0.04619</td><td>0.04833</td><td>0.05013</td><td>0.1987</td><td>0.06169</td></tr>\n",
       "\t<tr><th scope=row>39</th><td> TRUE</td><td>14.99</td><td>25.20</td><td> 95.54</td><td> 698.8</td><td>0.09387</td><td>0.05131</td><td>0.02398</td><td>0.02899</td><td>0.1565</td><td>⋯</td><td>14.99</td><td>25.20</td><td> 95.54</td><td> 698.8</td><td>0.09387</td><td>0.05131</td><td>0.02398</td><td>0.02899</td><td>0.1565</td><td>0.05504</td></tr>\n",
       "\t<tr><th scope=row>40</th><td> TRUE</td><td>13.48</td><td>20.82</td><td> 88.40</td><td> 559.2</td><td>0.10160</td><td>0.12550</td><td>0.10630</td><td>0.05439</td><td>0.1720</td><td>⋯</td><td>15.53</td><td>26.02</td><td>107.30</td><td> 740.4</td><td>0.16100</td><td>0.42250</td><td>0.50300</td><td>0.22580</td><td>0.2807</td><td>0.10710</td></tr>\n",
       "\t<tr><th scope=row>42</th><td> TRUE</td><td>10.95</td><td>21.35</td><td> 71.90</td><td> 371.1</td><td>0.12270</td><td>0.12180</td><td>0.10440</td><td>0.05669</td><td>0.1895</td><td>⋯</td><td>12.84</td><td>35.34</td><td> 87.22</td><td> 514.0</td><td>0.19090</td><td>0.26980</td><td>0.40230</td><td>0.14240</td><td>0.2964</td><td>0.09606</td></tr>\n",
       "\t<tr><th scope=row>43</th><td> TRUE</td><td>19.07</td><td>24.81</td><td>128.30</td><td>1104.0</td><td>0.09081</td><td>0.21900</td><td>0.21070</td><td>0.09961</td><td>0.2310</td><td>⋯</td><td>24.09</td><td>33.17</td><td>177.40</td><td>1651.0</td><td>0.12470</td><td>0.74440</td><td>0.72420</td><td>0.24930</td><td>0.4670</td><td>0.10380</td></tr>\n",
       "\t<tr><th scope=row>44</th><td> TRUE</td><td>13.28</td><td>20.28</td><td> 87.32</td><td> 545.2</td><td>0.10410</td><td>0.14360</td><td>0.09847</td><td>0.06158</td><td>0.1974</td><td>⋯</td><td>17.38</td><td>28.00</td><td>113.10</td><td> 907.2</td><td>0.15300</td><td>0.37240</td><td>0.36640</td><td>0.14920</td><td>0.3739</td><td>0.10270</td></tr>\n",
       "\t<tr><th scope=row>45</th><td> TRUE</td><td>13.17</td><td>21.81</td><td> 85.42</td><td> 531.5</td><td>0.09714</td><td>0.10470</td><td>0.08259</td><td>0.05252</td><td>0.1746</td><td>⋯</td><td>16.23</td><td>29.89</td><td>105.50</td><td> 740.7</td><td>0.15030</td><td>0.39040</td><td>0.37280</td><td>0.16070</td><td>0.3693</td><td>0.09618</td></tr>\n",
       "\t<tr><th scope=row>46</th><td> TRUE</td><td>18.65</td><td>17.60</td><td>123.70</td><td>1076.0</td><td>0.10990</td><td>0.16860</td><td>0.19740</td><td>0.10090</td><td>0.1907</td><td>⋯</td><td>22.82</td><td>21.32</td><td>150.60</td><td>1567.0</td><td>0.16790</td><td>0.50900</td><td>0.73450</td><td>0.23780</td><td>0.3799</td><td>0.09185</td></tr>\n",
       "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><th scope=row>525</th><td>FALSE</td><td> 9.847</td><td>15.68</td><td> 63.00</td><td> 293.2</td><td>0.09492</td><td>0.08419</td><td>0.02330</td><td>0.024160</td><td>0.1387</td><td>⋯</td><td>11.240</td><td>22.99</td><td> 74.32</td><td> 376.5</td><td>0.14190</td><td>0.22430</td><td>0.08434</td><td>0.06528</td><td>0.2502</td><td>0.09209</td></tr>\n",
       "\t<tr><th scope=row>526</th><td>FALSE</td><td> 8.571</td><td>13.10</td><td> 54.53</td><td> 221.3</td><td>0.10360</td><td>0.07632</td><td>0.02565</td><td>0.015100</td><td>0.1678</td><td>⋯</td><td> 9.473</td><td>18.45</td><td> 63.30</td><td> 275.6</td><td>0.16410</td><td>0.22350</td><td>0.17540</td><td>0.08512</td><td>0.2983</td><td>0.10490</td></tr>\n",
       "\t<tr><th scope=row>528</th><td>FALSE</td><td>12.340</td><td>12.27</td><td> 78.94</td><td> 468.5</td><td>0.09003</td><td>0.06307</td><td>0.02958</td><td>0.026470</td><td>0.1689</td><td>⋯</td><td>13.610</td><td>19.27</td><td> 87.22</td><td> 564.9</td><td>0.12920</td><td>0.20740</td><td>0.17910</td><td>0.10700</td><td>0.3110</td><td>0.07592</td></tr>\n",
       "\t<tr><th scope=row>529</th><td>FALSE</td><td>13.940</td><td>13.17</td><td> 90.31</td><td> 594.2</td><td>0.12480</td><td>0.09755</td><td>0.10100</td><td>0.066150</td><td>0.1976</td><td>⋯</td><td>14.620</td><td>15.38</td><td> 94.52</td><td> 653.3</td><td>0.13940</td><td>0.13640</td><td>0.15590</td><td>0.10150</td><td>0.2160</td><td>0.07253</td></tr>\n",
       "\t<tr><th scope=row>531</th><td>FALSE</td><td>11.750</td><td>17.56</td><td> 75.89</td><td> 422.9</td><td>0.10730</td><td>0.09713</td><td>0.05282</td><td>0.044400</td><td>0.1598</td><td>⋯</td><td>13.500</td><td>27.98</td><td> 88.52</td><td> 552.3</td><td>0.13490</td><td>0.18540</td><td>0.13660</td><td>0.10100</td><td>0.2478</td><td>0.07757</td></tr>\n",
       "\t<tr><th scope=row>532</th><td>FALSE</td><td>11.670</td><td>20.02</td><td> 75.21</td><td> 416.2</td><td>0.10160</td><td>0.09453</td><td>0.04200</td><td>0.021570</td><td>0.1859</td><td>⋯</td><td>13.350</td><td>28.81</td><td> 87.00</td><td> 550.6</td><td>0.15500</td><td>0.29640</td><td>0.27580</td><td>0.08120</td><td>0.3206</td><td>0.08950</td></tr>\n",
       "\t<tr><th scope=row>533</th><td>FALSE</td><td>13.680</td><td>16.33</td><td> 87.76</td><td> 575.5</td><td>0.09277</td><td>0.07255</td><td>0.01752</td><td>0.018800</td><td>0.1631</td><td>⋯</td><td>15.850</td><td>20.20</td><td>101.60</td><td> 773.4</td><td>0.12640</td><td>0.15640</td><td>0.12060</td><td>0.08704</td><td>0.2806</td><td>0.07782</td></tr>\n",
       "\t<tr><th scope=row>534</th><td> TRUE</td><td>20.470</td><td>20.67</td><td>134.70</td><td>1299.0</td><td>0.09156</td><td>0.13130</td><td>0.15230</td><td>0.101500</td><td>0.2166</td><td>⋯</td><td>23.230</td><td>27.15</td><td>152.00</td><td>1645.0</td><td>0.10970</td><td>0.25340</td><td>0.30920</td><td>0.16130</td><td>0.3220</td><td>0.06386</td></tr>\n",
       "\t<tr><th scope=row>537</th><td> TRUE</td><td>14.270</td><td>22.55</td><td> 93.77</td><td> 629.8</td><td>0.10380</td><td>0.11540</td><td>0.14630</td><td>0.061390</td><td>0.1926</td><td>⋯</td><td>15.290</td><td>34.27</td><td>104.30</td><td> 728.3</td><td>0.13800</td><td>0.27330</td><td>0.42340</td><td>0.13620</td><td>0.2698</td><td>0.08351</td></tr>\n",
       "\t<tr><th scope=row>538</th><td>FALSE</td><td>11.690</td><td>24.44</td><td> 76.37</td><td> 406.4</td><td>0.12360</td><td>0.15520</td><td>0.04515</td><td>0.045310</td><td>0.2131</td><td>⋯</td><td>12.980</td><td>32.19</td><td> 86.12</td><td> 487.7</td><td>0.17680</td><td>0.32510</td><td>0.13950</td><td>0.13080</td><td>0.2803</td><td>0.09970</td></tr>\n",
       "\t<tr><th scope=row>540</th><td>FALSE</td><td> 7.691</td><td>25.44</td><td> 48.34</td><td> 170.4</td><td>0.08668</td><td>0.11990</td><td>0.09252</td><td>0.013640</td><td>0.2037</td><td>⋯</td><td> 8.678</td><td>31.89</td><td> 54.49</td><td> 223.6</td><td>0.15960</td><td>0.30640</td><td>0.33930</td><td>0.05000</td><td>0.2790</td><td>0.10660</td></tr>\n",
       "\t<tr><th scope=row>541</th><td>FALSE</td><td>11.540</td><td>14.44</td><td> 74.65</td><td> 402.9</td><td>0.09984</td><td>0.11200</td><td>0.06737</td><td>0.025940</td><td>0.1818</td><td>⋯</td><td>12.260</td><td>19.68</td><td> 78.78</td><td> 457.8</td><td>0.13450</td><td>0.21180</td><td>0.17970</td><td>0.06918</td><td>0.2329</td><td>0.08134</td></tr>\n",
       "\t<tr><th scope=row>542</th><td>FALSE</td><td>14.470</td><td>24.99</td><td> 95.81</td><td> 656.4</td><td>0.08837</td><td>0.12300</td><td>0.10090</td><td>0.038900</td><td>0.1872</td><td>⋯</td><td>16.220</td><td>31.73</td><td>113.50</td><td> 808.9</td><td>0.13400</td><td>0.42020</td><td>0.40400</td><td>0.12050</td><td>0.3187</td><td>0.10230</td></tr>\n",
       "\t<tr><th scope=row>545</th><td>FALSE</td><td>13.870</td><td>20.70</td><td> 89.77</td><td> 584.8</td><td>0.09578</td><td>0.10180</td><td>0.03688</td><td>0.023690</td><td>0.1620</td><td>⋯</td><td>15.050</td><td>24.75</td><td> 99.17</td><td> 688.6</td><td>0.12640</td><td>0.20370</td><td>0.13770</td><td>0.06845</td><td>0.2249</td><td>0.08492</td></tr>\n",
       "\t<tr><th scope=row>547</th><td>FALSE</td><td>10.320</td><td>16.35</td><td> 65.31</td><td> 324.9</td><td>0.09434</td><td>0.04994</td><td>0.01012</td><td>0.005495</td><td>0.1885</td><td>⋯</td><td>11.250</td><td>21.77</td><td> 71.12</td><td> 384.9</td><td>0.12850</td><td>0.08842</td><td>0.04384</td><td>0.02381</td><td>0.2681</td><td>0.07399</td></tr>\n",
       "\t<tr><th scope=row>548</th><td>FALSE</td><td>10.260</td><td>16.58</td><td> 65.85</td><td> 320.8</td><td>0.08877</td><td>0.08066</td><td>0.04358</td><td>0.024380</td><td>0.1669</td><td>⋯</td><td>10.830</td><td>22.04</td><td> 71.08</td><td> 357.4</td><td>0.14610</td><td>0.22460</td><td>0.17830</td><td>0.08333</td><td>0.2691</td><td>0.09479</td></tr>\n",
       "\t<tr><th scope=row>549</th><td>FALSE</td><td> 9.683</td><td>19.34</td><td> 61.05</td><td> 285.7</td><td>0.08491</td><td>0.05030</td><td>0.02337</td><td>0.009615</td><td>0.1580</td><td>⋯</td><td>10.930</td><td>25.59</td><td> 69.10</td><td> 364.2</td><td>0.11990</td><td>0.09546</td><td>0.09350</td><td>0.03846</td><td>0.2552</td><td>0.07920</td></tr>\n",
       "\t<tr><th scope=row>551</th><td>FALSE</td><td>10.860</td><td>21.48</td><td> 68.51</td><td> 360.5</td><td>0.07431</td><td>0.04227</td><td>0.00000</td><td>0.000000</td><td>0.1661</td><td>⋯</td><td>11.660</td><td>24.77</td><td> 74.08</td><td> 412.3</td><td>0.10010</td><td>0.07348</td><td>0.00000</td><td>0.00000</td><td>0.2458</td><td>0.06592</td></tr>\n",
       "\t<tr><th scope=row>553</th><td>FALSE</td><td>12.770</td><td>29.43</td><td> 81.35</td><td> 507.9</td><td>0.08276</td><td>0.04234</td><td>0.01997</td><td>0.014990</td><td>0.1539</td><td>⋯</td><td>13.870</td><td>36.00</td><td> 88.10</td><td> 594.7</td><td>0.12340</td><td>0.10640</td><td>0.08653</td><td>0.06498</td><td>0.2407</td><td>0.06484</td></tr>\n",
       "\t<tr><th scope=row>554</th><td>FALSE</td><td> 9.333</td><td>21.94</td><td> 59.01</td><td> 264.0</td><td>0.09240</td><td>0.05605</td><td>0.03996</td><td>0.012820</td><td>0.1692</td><td>⋯</td><td> 9.845</td><td>25.05</td><td> 62.86</td><td> 295.8</td><td>0.11030</td><td>0.08298</td><td>0.07993</td><td>0.02564</td><td>0.2435</td><td>0.07393</td></tr>\n",
       "\t<tr><th scope=row>556</th><td>FALSE</td><td>10.290</td><td>27.61</td><td> 65.67</td><td> 321.4</td><td>0.09030</td><td>0.07658</td><td>0.05999</td><td>0.027380</td><td>0.1593</td><td>⋯</td><td>10.840</td><td>34.91</td><td> 69.57</td><td> 357.6</td><td>0.13840</td><td>0.17100</td><td>0.20000</td><td>0.09127</td><td>0.2226</td><td>0.08283</td></tr>\n",
       "\t<tr><th scope=row>558</th><td>FALSE</td><td> 9.423</td><td>27.88</td><td> 59.26</td><td> 271.3</td><td>0.08123</td><td>0.04971</td><td>0.00000</td><td>0.000000</td><td>0.1742</td><td>⋯</td><td>10.490</td><td>34.24</td><td> 66.50</td><td> 330.6</td><td>0.10730</td><td>0.07158</td><td>0.00000</td><td>0.00000</td><td>0.2475</td><td>0.06969</td></tr>\n",
       "\t<tr><th scope=row>561</th><td>FALSE</td><td>14.050</td><td>27.15</td><td> 91.38</td><td> 600.4</td><td>0.09929</td><td>0.11260</td><td>0.04462</td><td>0.043040</td><td>0.1537</td><td>⋯</td><td>15.300</td><td>33.17</td><td>100.20</td><td> 706.7</td><td>0.12410</td><td>0.22640</td><td>0.13260</td><td>0.10480</td><td>0.2250</td><td>0.08321</td></tr>\n",
       "\t<tr><th scope=row>562</th><td>FALSE</td><td>11.200</td><td>29.37</td><td> 70.67</td><td> 386.0</td><td>0.07449</td><td>0.03558</td><td>0.00000</td><td>0.000000</td><td>0.1060</td><td>⋯</td><td>11.920</td><td>38.30</td><td> 75.19</td><td> 439.6</td><td>0.09267</td><td>0.05494</td><td>0.00000</td><td>0.00000</td><td>0.1566</td><td>0.05905</td></tr>\n",
       "\t<tr><th scope=row>563</th><td> TRUE</td><td>15.220</td><td>30.62</td><td>103.40</td><td> 716.9</td><td>0.10480</td><td>0.20870</td><td>0.25500</td><td>0.094290</td><td>0.2128</td><td>⋯</td><td>17.520</td><td>42.79</td><td>128.70</td><td> 915.0</td><td>0.14170</td><td>0.79170</td><td>1.17000</td><td>0.23560</td><td>0.4089</td><td>0.14090</td></tr>\n",
       "\t<tr><th scope=row>564</th><td> TRUE</td><td>20.920</td><td>25.09</td><td>143.00</td><td>1347.0</td><td>0.10990</td><td>0.22360</td><td>0.31740</td><td>0.147400</td><td>0.2149</td><td>⋯</td><td>24.290</td><td>29.41</td><td>179.10</td><td>1819.0</td><td>0.14070</td><td>0.41860</td><td>0.65990</td><td>0.25420</td><td>0.2929</td><td>0.09873</td></tr>\n",
       "\t<tr><th scope=row>565</th><td> TRUE</td><td>21.560</td><td>22.39</td><td>142.00</td><td>1479.0</td><td>0.11100</td><td>0.11590</td><td>0.24390</td><td>0.138900</td><td>0.1726</td><td>⋯</td><td>25.450</td><td>26.40</td><td>166.10</td><td>2027.0</td><td>0.14100</td><td>0.21130</td><td>0.41070</td><td>0.22160</td><td>0.2060</td><td>0.07115</td></tr>\n",
       "\t<tr><th scope=row>567</th><td> TRUE</td><td>16.600</td><td>28.08</td><td>108.30</td><td> 858.1</td><td>0.08455</td><td>0.10230</td><td>0.09251</td><td>0.053020</td><td>0.1590</td><td>⋯</td><td>18.980</td><td>34.12</td><td>126.70</td><td>1124.0</td><td>0.11390</td><td>0.30940</td><td>0.34030</td><td>0.14180</td><td>0.2218</td><td>0.07820</td></tr>\n",
       "\t<tr><th scope=row>568</th><td> TRUE</td><td>20.600</td><td>29.33</td><td>140.10</td><td>1265.0</td><td>0.11780</td><td>0.27700</td><td>0.35140</td><td>0.152000</td><td>0.2397</td><td>⋯</td><td>25.740</td><td>39.42</td><td>184.60</td><td>1821.0</td><td>0.16500</td><td>0.86810</td><td>0.93870</td><td>0.26500</td><td>0.4087</td><td>0.12400</td></tr>\n",
       "\t<tr><th scope=row>569</th><td>FALSE</td><td> 7.760</td><td>24.54</td><td> 47.92</td><td> 181.0</td><td>0.05263</td><td>0.04362</td><td>0.00000</td><td>0.000000</td><td>0.1587</td><td>⋯</td><td> 9.456</td><td>30.37</td><td> 59.16</td><td> 268.6</td><td>0.08996</td><td>0.06444</td><td>0.00000</td><td>0.00000</td><td>0.2871</td><td>0.07039</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 399 × 31\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & train\\_data\\_y & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & compactness\\_mean & concavity\\_mean & concave.points\\_mean & symmetry\\_mean & ⋯ & radius\\_worst & texture\\_worst & perimeter\\_worst & area\\_worst & smoothness\\_worst & compactness\\_worst & concavity\\_worst & concave.points\\_worst & symmetry\\_worst & fractal\\_dimension\\_worst\\\\\n",
       "  & <lgl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t2 &  TRUE & 20.57 & 17.77 & 132.90 & 1326.0 & 0.08474 & 0.07864 & 0.08690 & 0.07017 & 0.1812 & ⋯ & 24.99 & 23.41 & 158.80 & 1956.0 & 0.12380 & 0.18660 & 0.24160 & 0.18600 & 0.2750 & 0.08902\\\\\n",
       "\t3 &  TRUE & 19.69 & 21.25 & 130.00 & 1203.0 & 0.10960 & 0.15990 & 0.19740 & 0.12790 & 0.2069 & ⋯ & 23.57 & 25.53 & 152.50 & 1709.0 & 0.14440 & 0.42450 & 0.45040 & 0.24300 & 0.3613 & 0.08758\\\\\n",
       "\t4 &  TRUE & 11.42 & 20.38 &  77.58 &  386.1 & 0.14250 & 0.28390 & 0.24140 & 0.10520 & 0.2597 & ⋯ & 14.91 & 26.50 &  98.87 &  567.7 & 0.20980 & 0.86630 & 0.68690 & 0.25750 & 0.6638 & 0.17300\\\\\n",
       "\t10 &  TRUE & 12.46 & 24.04 &  83.97 &  475.9 & 0.11860 & 0.23960 & 0.22730 & 0.08543 & 0.2030 & ⋯ & 15.09 & 40.68 &  97.65 &  711.4 & 0.18530 & 1.05800 & 1.10500 & 0.22100 & 0.4366 & 0.20750\\\\\n",
       "\t11 &  TRUE & 16.02 & 23.24 & 102.70 &  797.8 & 0.08206 & 0.06669 & 0.03299 & 0.03323 & 0.1528 & ⋯ & 19.19 & 33.88 & 123.80 & 1150.0 & 0.11810 & 0.15510 & 0.14590 & 0.09975 & 0.2948 & 0.08452\\\\\n",
       "\t13 &  TRUE & 19.17 & 24.80 & 132.40 & 1123.0 & 0.09740 & 0.24580 & 0.20650 & 0.11180 & 0.2397 & ⋯ & 20.96 & 29.94 & 151.70 & 1332.0 & 0.10370 & 0.39030 & 0.36390 & 0.17670 & 0.3176 & 0.10230\\\\\n",
       "\t14 &  TRUE & 15.85 & 23.95 & 103.70 &  782.7 & 0.08401 & 0.10020 & 0.09938 & 0.05364 & 0.1847 & ⋯ & 16.84 & 27.66 & 112.00 &  876.5 & 0.11310 & 0.19240 & 0.23220 & 0.11190 & 0.2809 & 0.06287\\\\\n",
       "\t15 &  TRUE & 13.73 & 22.61 &  93.60 &  578.3 & 0.11310 & 0.22930 & 0.21280 & 0.08025 & 0.2069 & ⋯ & 15.03 & 32.01 & 108.80 &  697.7 & 0.16510 & 0.77250 & 0.69430 & 0.22080 & 0.3596 & 0.14310\\\\\n",
       "\t16 &  TRUE & 14.54 & 27.54 &  96.73 &  658.8 & 0.11390 & 0.15950 & 0.16390 & 0.07364 & 0.2303 & ⋯ & 17.46 & 37.13 & 124.10 &  943.2 & 0.16780 & 0.65770 & 0.70260 & 0.17120 & 0.4218 & 0.13410\\\\\n",
       "\t17 &  TRUE & 14.68 & 20.13 &  94.74 &  684.5 & 0.09867 & 0.07200 & 0.07395 & 0.05259 & 0.1586 & ⋯ & 19.07 & 30.88 & 123.40 & 1138.0 & 0.14640 & 0.18710 & 0.29140 & 0.16090 & 0.3029 & 0.08216\\\\\n",
       "\t18 &  TRUE & 16.13 & 20.68 & 108.10 &  798.8 & 0.11700 & 0.20220 & 0.17220 & 0.10280 & 0.2164 & ⋯ & 20.96 & 31.48 & 136.80 & 1315.0 & 0.17890 & 0.42330 & 0.47840 & 0.20730 & 0.3706 & 0.11420\\\\\n",
       "\t19 &  TRUE & 19.81 & 22.15 & 130.00 & 1260.0 & 0.09831 & 0.10270 & 0.14790 & 0.09498 & 0.1582 & ⋯ & 27.32 & 30.88 & 186.80 & 2398.0 & 0.15120 & 0.31500 & 0.53720 & 0.23880 & 0.2768 & 0.07615\\\\\n",
       "\t23 &  TRUE & 15.34 & 14.26 & 102.50 &  704.4 & 0.10730 & 0.21350 & 0.20770 & 0.09756 & 0.2521 & ⋯ & 18.07 & 19.08 & 125.10 &  980.9 & 0.13900 & 0.59540 & 0.63050 & 0.23930 & 0.4667 & 0.09946\\\\\n",
       "\t24 &  TRUE & 21.16 & 23.04 & 137.20 & 1404.0 & 0.09428 & 0.10220 & 0.10970 & 0.08632 & 0.1769 & ⋯ & 29.17 & 35.59 & 188.00 & 2615.0 & 0.14010 & 0.26000 & 0.31550 & 0.20090 & 0.2822 & 0.07526\\\\\n",
       "\t27 &  TRUE & 14.58 & 21.53 &  97.41 &  644.8 & 0.10540 & 0.18680 & 0.14250 & 0.08783 & 0.2252 & ⋯ & 17.62 & 33.21 & 122.40 &  896.9 & 0.15250 & 0.66430 & 0.55390 & 0.27010 & 0.4264 & 0.12750\\\\\n",
       "\t28 &  TRUE & 18.61 & 20.25 & 122.10 & 1094.0 & 0.09440 & 0.10660 & 0.14900 & 0.07731 & 0.1697 & ⋯ & 21.31 & 27.26 & 139.90 & 1403.0 & 0.13380 & 0.21170 & 0.34460 & 0.14900 & 0.2341 & 0.07421\\\\\n",
       "\t29 &  TRUE & 15.30 & 25.27 & 102.40 &  732.4 & 0.10820 & 0.16970 & 0.16830 & 0.08751 & 0.1926 & ⋯ & 20.27 & 36.71 & 149.30 & 1269.0 & 0.16410 & 0.61100 & 0.63350 & 0.20240 & 0.4027 & 0.09876\\\\\n",
       "\t32 &  TRUE & 11.84 & 18.70 &  77.93 &  440.6 & 0.11090 & 0.15160 & 0.12180 & 0.05182 & 0.2301 & ⋯ & 16.82 & 28.12 & 119.40 &  888.7 & 0.16370 & 0.57750 & 0.69560 & 0.15460 & 0.4761 & 0.14020\\\\\n",
       "\t33 &  TRUE & 17.02 & 23.98 & 112.80 &  899.3 & 0.11970 & 0.14960 & 0.24170 & 0.12030 & 0.2248 & ⋯ & 20.88 & 32.09 & 136.10 & 1344.0 & 0.16340 & 0.35590 & 0.55880 & 0.18470 & 0.3530 & 0.08482\\\\\n",
       "\t34 &  TRUE & 19.27 & 26.47 & 127.90 & 1162.0 & 0.09401 & 0.17190 & 0.16570 & 0.07593 & 0.1853 & ⋯ & 24.15 & 30.90 & 161.40 & 1813.0 & 0.15090 & 0.65900 & 0.60910 & 0.17850 & 0.3672 & 0.11230\\\\\n",
       "\t36 &  TRUE & 16.74 & 21.59 & 110.10 &  869.5 & 0.09610 & 0.13360 & 0.13480 & 0.06018 & 0.1896 & ⋯ & 20.01 & 29.02 & 133.50 & 1229.0 & 0.15630 & 0.38350 & 0.54090 & 0.18130 & 0.4863 & 0.08633\\\\\n",
       "\t37 &  TRUE & 14.25 & 21.72 &  93.63 &  633.0 & 0.09823 & 0.10980 & 0.13190 & 0.05598 & 0.1885 & ⋯ & 15.89 & 30.36 & 116.20 &  799.6 & 0.14460 & 0.42380 & 0.51860 & 0.14470 & 0.3591 & 0.10140\\\\\n",
       "\t38 & FALSE & 13.03 & 18.42 &  82.61 &  523.8 & 0.08983 & 0.03766 & 0.02562 & 0.02923 & 0.1467 & ⋯ & 13.30 & 22.81 &  84.46 &  545.9 & 0.09701 & 0.04619 & 0.04833 & 0.05013 & 0.1987 & 0.06169\\\\\n",
       "\t39 &  TRUE & 14.99 & 25.20 &  95.54 &  698.8 & 0.09387 & 0.05131 & 0.02398 & 0.02899 & 0.1565 & ⋯ & 14.99 & 25.20 &  95.54 &  698.8 & 0.09387 & 0.05131 & 0.02398 & 0.02899 & 0.1565 & 0.05504\\\\\n",
       "\t40 &  TRUE & 13.48 & 20.82 &  88.40 &  559.2 & 0.10160 & 0.12550 & 0.10630 & 0.05439 & 0.1720 & ⋯ & 15.53 & 26.02 & 107.30 &  740.4 & 0.16100 & 0.42250 & 0.50300 & 0.22580 & 0.2807 & 0.10710\\\\\n",
       "\t42 &  TRUE & 10.95 & 21.35 &  71.90 &  371.1 & 0.12270 & 0.12180 & 0.10440 & 0.05669 & 0.1895 & ⋯ & 12.84 & 35.34 &  87.22 &  514.0 & 0.19090 & 0.26980 & 0.40230 & 0.14240 & 0.2964 & 0.09606\\\\\n",
       "\t43 &  TRUE & 19.07 & 24.81 & 128.30 & 1104.0 & 0.09081 & 0.21900 & 0.21070 & 0.09961 & 0.2310 & ⋯ & 24.09 & 33.17 & 177.40 & 1651.0 & 0.12470 & 0.74440 & 0.72420 & 0.24930 & 0.4670 & 0.10380\\\\\n",
       "\t44 &  TRUE & 13.28 & 20.28 &  87.32 &  545.2 & 0.10410 & 0.14360 & 0.09847 & 0.06158 & 0.1974 & ⋯ & 17.38 & 28.00 & 113.10 &  907.2 & 0.15300 & 0.37240 & 0.36640 & 0.14920 & 0.3739 & 0.10270\\\\\n",
       "\t45 &  TRUE & 13.17 & 21.81 &  85.42 &  531.5 & 0.09714 & 0.10470 & 0.08259 & 0.05252 & 0.1746 & ⋯ & 16.23 & 29.89 & 105.50 &  740.7 & 0.15030 & 0.39040 & 0.37280 & 0.16070 & 0.3693 & 0.09618\\\\\n",
       "\t46 &  TRUE & 18.65 & 17.60 & 123.70 & 1076.0 & 0.10990 & 0.16860 & 0.19740 & 0.10090 & 0.1907 & ⋯ & 22.82 & 21.32 & 150.60 & 1567.0 & 0.16790 & 0.50900 & 0.73450 & 0.23780 & 0.3799 & 0.09185\\\\\n",
       "\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t525 & FALSE &  9.847 & 15.68 &  63.00 &  293.2 & 0.09492 & 0.08419 & 0.02330 & 0.024160 & 0.1387 & ⋯ & 11.240 & 22.99 &  74.32 &  376.5 & 0.14190 & 0.22430 & 0.08434 & 0.06528 & 0.2502 & 0.09209\\\\\n",
       "\t526 & FALSE &  8.571 & 13.10 &  54.53 &  221.3 & 0.10360 & 0.07632 & 0.02565 & 0.015100 & 0.1678 & ⋯ &  9.473 & 18.45 &  63.30 &  275.6 & 0.16410 & 0.22350 & 0.17540 & 0.08512 & 0.2983 & 0.10490\\\\\n",
       "\t528 & FALSE & 12.340 & 12.27 &  78.94 &  468.5 & 0.09003 & 0.06307 & 0.02958 & 0.026470 & 0.1689 & ⋯ & 13.610 & 19.27 &  87.22 &  564.9 & 0.12920 & 0.20740 & 0.17910 & 0.10700 & 0.3110 & 0.07592\\\\\n",
       "\t529 & FALSE & 13.940 & 13.17 &  90.31 &  594.2 & 0.12480 & 0.09755 & 0.10100 & 0.066150 & 0.1976 & ⋯ & 14.620 & 15.38 &  94.52 &  653.3 & 0.13940 & 0.13640 & 0.15590 & 0.10150 & 0.2160 & 0.07253\\\\\n",
       "\t531 & FALSE & 11.750 & 17.56 &  75.89 &  422.9 & 0.10730 & 0.09713 & 0.05282 & 0.044400 & 0.1598 & ⋯ & 13.500 & 27.98 &  88.52 &  552.3 & 0.13490 & 0.18540 & 0.13660 & 0.10100 & 0.2478 & 0.07757\\\\\n",
       "\t532 & FALSE & 11.670 & 20.02 &  75.21 &  416.2 & 0.10160 & 0.09453 & 0.04200 & 0.021570 & 0.1859 & ⋯ & 13.350 & 28.81 &  87.00 &  550.6 & 0.15500 & 0.29640 & 0.27580 & 0.08120 & 0.3206 & 0.08950\\\\\n",
       "\t533 & FALSE & 13.680 & 16.33 &  87.76 &  575.5 & 0.09277 & 0.07255 & 0.01752 & 0.018800 & 0.1631 & ⋯ & 15.850 & 20.20 & 101.60 &  773.4 & 0.12640 & 0.15640 & 0.12060 & 0.08704 & 0.2806 & 0.07782\\\\\n",
       "\t534 &  TRUE & 20.470 & 20.67 & 134.70 & 1299.0 & 0.09156 & 0.13130 & 0.15230 & 0.101500 & 0.2166 & ⋯ & 23.230 & 27.15 & 152.00 & 1645.0 & 0.10970 & 0.25340 & 0.30920 & 0.16130 & 0.3220 & 0.06386\\\\\n",
       "\t537 &  TRUE & 14.270 & 22.55 &  93.77 &  629.8 & 0.10380 & 0.11540 & 0.14630 & 0.061390 & 0.1926 & ⋯ & 15.290 & 34.27 & 104.30 &  728.3 & 0.13800 & 0.27330 & 0.42340 & 0.13620 & 0.2698 & 0.08351\\\\\n",
       "\t538 & FALSE & 11.690 & 24.44 &  76.37 &  406.4 & 0.12360 & 0.15520 & 0.04515 & 0.045310 & 0.2131 & ⋯ & 12.980 & 32.19 &  86.12 &  487.7 & 0.17680 & 0.32510 & 0.13950 & 0.13080 & 0.2803 & 0.09970\\\\\n",
       "\t540 & FALSE &  7.691 & 25.44 &  48.34 &  170.4 & 0.08668 & 0.11990 & 0.09252 & 0.013640 & 0.2037 & ⋯ &  8.678 & 31.89 &  54.49 &  223.6 & 0.15960 & 0.30640 & 0.33930 & 0.05000 & 0.2790 & 0.10660\\\\\n",
       "\t541 & FALSE & 11.540 & 14.44 &  74.65 &  402.9 & 0.09984 & 0.11200 & 0.06737 & 0.025940 & 0.1818 & ⋯ & 12.260 & 19.68 &  78.78 &  457.8 & 0.13450 & 0.21180 & 0.17970 & 0.06918 & 0.2329 & 0.08134\\\\\n",
       "\t542 & FALSE & 14.470 & 24.99 &  95.81 &  656.4 & 0.08837 & 0.12300 & 0.10090 & 0.038900 & 0.1872 & ⋯ & 16.220 & 31.73 & 113.50 &  808.9 & 0.13400 & 0.42020 & 0.40400 & 0.12050 & 0.3187 & 0.10230\\\\\n",
       "\t545 & FALSE & 13.870 & 20.70 &  89.77 &  584.8 & 0.09578 & 0.10180 & 0.03688 & 0.023690 & 0.1620 & ⋯ & 15.050 & 24.75 &  99.17 &  688.6 & 0.12640 & 0.20370 & 0.13770 & 0.06845 & 0.2249 & 0.08492\\\\\n",
       "\t547 & FALSE & 10.320 & 16.35 &  65.31 &  324.9 & 0.09434 & 0.04994 & 0.01012 & 0.005495 & 0.1885 & ⋯ & 11.250 & 21.77 &  71.12 &  384.9 & 0.12850 & 0.08842 & 0.04384 & 0.02381 & 0.2681 & 0.07399\\\\\n",
       "\t548 & FALSE & 10.260 & 16.58 &  65.85 &  320.8 & 0.08877 & 0.08066 & 0.04358 & 0.024380 & 0.1669 & ⋯ & 10.830 & 22.04 &  71.08 &  357.4 & 0.14610 & 0.22460 & 0.17830 & 0.08333 & 0.2691 & 0.09479\\\\\n",
       "\t549 & FALSE &  9.683 & 19.34 &  61.05 &  285.7 & 0.08491 & 0.05030 & 0.02337 & 0.009615 & 0.1580 & ⋯ & 10.930 & 25.59 &  69.10 &  364.2 & 0.11990 & 0.09546 & 0.09350 & 0.03846 & 0.2552 & 0.07920\\\\\n",
       "\t551 & FALSE & 10.860 & 21.48 &  68.51 &  360.5 & 0.07431 & 0.04227 & 0.00000 & 0.000000 & 0.1661 & ⋯ & 11.660 & 24.77 &  74.08 &  412.3 & 0.10010 & 0.07348 & 0.00000 & 0.00000 & 0.2458 & 0.06592\\\\\n",
       "\t553 & FALSE & 12.770 & 29.43 &  81.35 &  507.9 & 0.08276 & 0.04234 & 0.01997 & 0.014990 & 0.1539 & ⋯ & 13.870 & 36.00 &  88.10 &  594.7 & 0.12340 & 0.10640 & 0.08653 & 0.06498 & 0.2407 & 0.06484\\\\\n",
       "\t554 & FALSE &  9.333 & 21.94 &  59.01 &  264.0 & 0.09240 & 0.05605 & 0.03996 & 0.012820 & 0.1692 & ⋯ &  9.845 & 25.05 &  62.86 &  295.8 & 0.11030 & 0.08298 & 0.07993 & 0.02564 & 0.2435 & 0.07393\\\\\n",
       "\t556 & FALSE & 10.290 & 27.61 &  65.67 &  321.4 & 0.09030 & 0.07658 & 0.05999 & 0.027380 & 0.1593 & ⋯ & 10.840 & 34.91 &  69.57 &  357.6 & 0.13840 & 0.17100 & 0.20000 & 0.09127 & 0.2226 & 0.08283\\\\\n",
       "\t558 & FALSE &  9.423 & 27.88 &  59.26 &  271.3 & 0.08123 & 0.04971 & 0.00000 & 0.000000 & 0.1742 & ⋯ & 10.490 & 34.24 &  66.50 &  330.6 & 0.10730 & 0.07158 & 0.00000 & 0.00000 & 0.2475 & 0.06969\\\\\n",
       "\t561 & FALSE & 14.050 & 27.15 &  91.38 &  600.4 & 0.09929 & 0.11260 & 0.04462 & 0.043040 & 0.1537 & ⋯ & 15.300 & 33.17 & 100.20 &  706.7 & 0.12410 & 0.22640 & 0.13260 & 0.10480 & 0.2250 & 0.08321\\\\\n",
       "\t562 & FALSE & 11.200 & 29.37 &  70.67 &  386.0 & 0.07449 & 0.03558 & 0.00000 & 0.000000 & 0.1060 & ⋯ & 11.920 & 38.30 &  75.19 &  439.6 & 0.09267 & 0.05494 & 0.00000 & 0.00000 & 0.1566 & 0.05905\\\\\n",
       "\t563 &  TRUE & 15.220 & 30.62 & 103.40 &  716.9 & 0.10480 & 0.20870 & 0.25500 & 0.094290 & 0.2128 & ⋯ & 17.520 & 42.79 & 128.70 &  915.0 & 0.14170 & 0.79170 & 1.17000 & 0.23560 & 0.4089 & 0.14090\\\\\n",
       "\t564 &  TRUE & 20.920 & 25.09 & 143.00 & 1347.0 & 0.10990 & 0.22360 & 0.31740 & 0.147400 & 0.2149 & ⋯ & 24.290 & 29.41 & 179.10 & 1819.0 & 0.14070 & 0.41860 & 0.65990 & 0.25420 & 0.2929 & 0.09873\\\\\n",
       "\t565 &  TRUE & 21.560 & 22.39 & 142.00 & 1479.0 & 0.11100 & 0.11590 & 0.24390 & 0.138900 & 0.1726 & ⋯ & 25.450 & 26.40 & 166.10 & 2027.0 & 0.14100 & 0.21130 & 0.41070 & 0.22160 & 0.2060 & 0.07115\\\\\n",
       "\t567 &  TRUE & 16.600 & 28.08 & 108.30 &  858.1 & 0.08455 & 0.10230 & 0.09251 & 0.053020 & 0.1590 & ⋯ & 18.980 & 34.12 & 126.70 & 1124.0 & 0.11390 & 0.30940 & 0.34030 & 0.14180 & 0.2218 & 0.07820\\\\\n",
       "\t568 &  TRUE & 20.600 & 29.33 & 140.10 & 1265.0 & 0.11780 & 0.27700 & 0.35140 & 0.152000 & 0.2397 & ⋯ & 25.740 & 39.42 & 184.60 & 1821.0 & 0.16500 & 0.86810 & 0.93870 & 0.26500 & 0.4087 & 0.12400\\\\\n",
       "\t569 & FALSE &  7.760 & 24.54 &  47.92 &  181.0 & 0.05263 & 0.04362 & 0.00000 & 0.000000 & 0.1587 & ⋯ &  9.456 & 30.37 &  59.16 &  268.6 & 0.08996 & 0.06444 & 0.00000 & 0.00000 & 0.2871 & 0.07039\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 399 × 31\n",
       "\n",
       "| <!--/--> | train_data_y &lt;lgl&gt; | radius_mean &lt;dbl&gt; | texture_mean &lt;dbl&gt; | perimeter_mean &lt;dbl&gt; | area_mean &lt;dbl&gt; | smoothness_mean &lt;dbl&gt; | compactness_mean &lt;dbl&gt; | concavity_mean &lt;dbl&gt; | concave.points_mean &lt;dbl&gt; | symmetry_mean &lt;dbl&gt; | ⋯ ⋯ | radius_worst &lt;dbl&gt; | texture_worst &lt;dbl&gt; | perimeter_worst &lt;dbl&gt; | area_worst &lt;dbl&gt; | smoothness_worst &lt;dbl&gt; | compactness_worst &lt;dbl&gt; | concavity_worst &lt;dbl&gt; | concave.points_worst &lt;dbl&gt; | symmetry_worst &lt;dbl&gt; | fractal_dimension_worst &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2 |  TRUE | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.08690 | 0.07017 | 0.1812 | ⋯ | 24.99 | 23.41 | 158.80 | 1956.0 | 0.12380 | 0.18660 | 0.24160 | 0.18600 | 0.2750 | 0.08902 |\n",
       "| 3 |  TRUE | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.19740 | 0.12790 | 0.2069 | ⋯ | 23.57 | 25.53 | 152.50 | 1709.0 | 0.14440 | 0.42450 | 0.45040 | 0.24300 | 0.3613 | 0.08758 |\n",
       "| 4 |  TRUE | 11.42 | 20.38 |  77.58 |  386.1 | 0.14250 | 0.28390 | 0.24140 | 0.10520 | 0.2597 | ⋯ | 14.91 | 26.50 |  98.87 |  567.7 | 0.20980 | 0.86630 | 0.68690 | 0.25750 | 0.6638 | 0.17300 |\n",
       "| 10 |  TRUE | 12.46 | 24.04 |  83.97 |  475.9 | 0.11860 | 0.23960 | 0.22730 | 0.08543 | 0.2030 | ⋯ | 15.09 | 40.68 |  97.65 |  711.4 | 0.18530 | 1.05800 | 1.10500 | 0.22100 | 0.4366 | 0.20750 |\n",
       "| 11 |  TRUE | 16.02 | 23.24 | 102.70 |  797.8 | 0.08206 | 0.06669 | 0.03299 | 0.03323 | 0.1528 | ⋯ | 19.19 | 33.88 | 123.80 | 1150.0 | 0.11810 | 0.15510 | 0.14590 | 0.09975 | 0.2948 | 0.08452 |\n",
       "| 13 |  TRUE | 19.17 | 24.80 | 132.40 | 1123.0 | 0.09740 | 0.24580 | 0.20650 | 0.11180 | 0.2397 | ⋯ | 20.96 | 29.94 | 151.70 | 1332.0 | 0.10370 | 0.39030 | 0.36390 | 0.17670 | 0.3176 | 0.10230 |\n",
       "| 14 |  TRUE | 15.85 | 23.95 | 103.70 |  782.7 | 0.08401 | 0.10020 | 0.09938 | 0.05364 | 0.1847 | ⋯ | 16.84 | 27.66 | 112.00 |  876.5 | 0.11310 | 0.19240 | 0.23220 | 0.11190 | 0.2809 | 0.06287 |\n",
       "| 15 |  TRUE | 13.73 | 22.61 |  93.60 |  578.3 | 0.11310 | 0.22930 | 0.21280 | 0.08025 | 0.2069 | ⋯ | 15.03 | 32.01 | 108.80 |  697.7 | 0.16510 | 0.77250 | 0.69430 | 0.22080 | 0.3596 | 0.14310 |\n",
       "| 16 |  TRUE | 14.54 | 27.54 |  96.73 |  658.8 | 0.11390 | 0.15950 | 0.16390 | 0.07364 | 0.2303 | ⋯ | 17.46 | 37.13 | 124.10 |  943.2 | 0.16780 | 0.65770 | 0.70260 | 0.17120 | 0.4218 | 0.13410 |\n",
       "| 17 |  TRUE | 14.68 | 20.13 |  94.74 |  684.5 | 0.09867 | 0.07200 | 0.07395 | 0.05259 | 0.1586 | ⋯ | 19.07 | 30.88 | 123.40 | 1138.0 | 0.14640 | 0.18710 | 0.29140 | 0.16090 | 0.3029 | 0.08216 |\n",
       "| 18 |  TRUE | 16.13 | 20.68 | 108.10 |  798.8 | 0.11700 | 0.20220 | 0.17220 | 0.10280 | 0.2164 | ⋯ | 20.96 | 31.48 | 136.80 | 1315.0 | 0.17890 | 0.42330 | 0.47840 | 0.20730 | 0.3706 | 0.11420 |\n",
       "| 19 |  TRUE | 19.81 | 22.15 | 130.00 | 1260.0 | 0.09831 | 0.10270 | 0.14790 | 0.09498 | 0.1582 | ⋯ | 27.32 | 30.88 | 186.80 | 2398.0 | 0.15120 | 0.31500 | 0.53720 | 0.23880 | 0.2768 | 0.07615 |\n",
       "| 23 |  TRUE | 15.34 | 14.26 | 102.50 |  704.4 | 0.10730 | 0.21350 | 0.20770 | 0.09756 | 0.2521 | ⋯ | 18.07 | 19.08 | 125.10 |  980.9 | 0.13900 | 0.59540 | 0.63050 | 0.23930 | 0.4667 | 0.09946 |\n",
       "| 24 |  TRUE | 21.16 | 23.04 | 137.20 | 1404.0 | 0.09428 | 0.10220 | 0.10970 | 0.08632 | 0.1769 | ⋯ | 29.17 | 35.59 | 188.00 | 2615.0 | 0.14010 | 0.26000 | 0.31550 | 0.20090 | 0.2822 | 0.07526 |\n",
       "| 27 |  TRUE | 14.58 | 21.53 |  97.41 |  644.8 | 0.10540 | 0.18680 | 0.14250 | 0.08783 | 0.2252 | ⋯ | 17.62 | 33.21 | 122.40 |  896.9 | 0.15250 | 0.66430 | 0.55390 | 0.27010 | 0.4264 | 0.12750 |\n",
       "| 28 |  TRUE | 18.61 | 20.25 | 122.10 | 1094.0 | 0.09440 | 0.10660 | 0.14900 | 0.07731 | 0.1697 | ⋯ | 21.31 | 27.26 | 139.90 | 1403.0 | 0.13380 | 0.21170 | 0.34460 | 0.14900 | 0.2341 | 0.07421 |\n",
       "| 29 |  TRUE | 15.30 | 25.27 | 102.40 |  732.4 | 0.10820 | 0.16970 | 0.16830 | 0.08751 | 0.1926 | ⋯ | 20.27 | 36.71 | 149.30 | 1269.0 | 0.16410 | 0.61100 | 0.63350 | 0.20240 | 0.4027 | 0.09876 |\n",
       "| 32 |  TRUE | 11.84 | 18.70 |  77.93 |  440.6 | 0.11090 | 0.15160 | 0.12180 | 0.05182 | 0.2301 | ⋯ | 16.82 | 28.12 | 119.40 |  888.7 | 0.16370 | 0.57750 | 0.69560 | 0.15460 | 0.4761 | 0.14020 |\n",
       "| 33 |  TRUE | 17.02 | 23.98 | 112.80 |  899.3 | 0.11970 | 0.14960 | 0.24170 | 0.12030 | 0.2248 | ⋯ | 20.88 | 32.09 | 136.10 | 1344.0 | 0.16340 | 0.35590 | 0.55880 | 0.18470 | 0.3530 | 0.08482 |\n",
       "| 34 |  TRUE | 19.27 | 26.47 | 127.90 | 1162.0 | 0.09401 | 0.17190 | 0.16570 | 0.07593 | 0.1853 | ⋯ | 24.15 | 30.90 | 161.40 | 1813.0 | 0.15090 | 0.65900 | 0.60910 | 0.17850 | 0.3672 | 0.11230 |\n",
       "| 36 |  TRUE | 16.74 | 21.59 | 110.10 |  869.5 | 0.09610 | 0.13360 | 0.13480 | 0.06018 | 0.1896 | ⋯ | 20.01 | 29.02 | 133.50 | 1229.0 | 0.15630 | 0.38350 | 0.54090 | 0.18130 | 0.4863 | 0.08633 |\n",
       "| 37 |  TRUE | 14.25 | 21.72 |  93.63 |  633.0 | 0.09823 | 0.10980 | 0.13190 | 0.05598 | 0.1885 | ⋯ | 15.89 | 30.36 | 116.20 |  799.6 | 0.14460 | 0.42380 | 0.51860 | 0.14470 | 0.3591 | 0.10140 |\n",
       "| 38 | FALSE | 13.03 | 18.42 |  82.61 |  523.8 | 0.08983 | 0.03766 | 0.02562 | 0.02923 | 0.1467 | ⋯ | 13.30 | 22.81 |  84.46 |  545.9 | 0.09701 | 0.04619 | 0.04833 | 0.05013 | 0.1987 | 0.06169 |\n",
       "| 39 |  TRUE | 14.99 | 25.20 |  95.54 |  698.8 | 0.09387 | 0.05131 | 0.02398 | 0.02899 | 0.1565 | ⋯ | 14.99 | 25.20 |  95.54 |  698.8 | 0.09387 | 0.05131 | 0.02398 | 0.02899 | 0.1565 | 0.05504 |\n",
       "| 40 |  TRUE | 13.48 | 20.82 |  88.40 |  559.2 | 0.10160 | 0.12550 | 0.10630 | 0.05439 | 0.1720 | ⋯ | 15.53 | 26.02 | 107.30 |  740.4 | 0.16100 | 0.42250 | 0.50300 | 0.22580 | 0.2807 | 0.10710 |\n",
       "| 42 |  TRUE | 10.95 | 21.35 |  71.90 |  371.1 | 0.12270 | 0.12180 | 0.10440 | 0.05669 | 0.1895 | ⋯ | 12.84 | 35.34 |  87.22 |  514.0 | 0.19090 | 0.26980 | 0.40230 | 0.14240 | 0.2964 | 0.09606 |\n",
       "| 43 |  TRUE | 19.07 | 24.81 | 128.30 | 1104.0 | 0.09081 | 0.21900 | 0.21070 | 0.09961 | 0.2310 | ⋯ | 24.09 | 33.17 | 177.40 | 1651.0 | 0.12470 | 0.74440 | 0.72420 | 0.24930 | 0.4670 | 0.10380 |\n",
       "| 44 |  TRUE | 13.28 | 20.28 |  87.32 |  545.2 | 0.10410 | 0.14360 | 0.09847 | 0.06158 | 0.1974 | ⋯ | 17.38 | 28.00 | 113.10 |  907.2 | 0.15300 | 0.37240 | 0.36640 | 0.14920 | 0.3739 | 0.10270 |\n",
       "| 45 |  TRUE | 13.17 | 21.81 |  85.42 |  531.5 | 0.09714 | 0.10470 | 0.08259 | 0.05252 | 0.1746 | ⋯ | 16.23 | 29.89 | 105.50 |  740.7 | 0.15030 | 0.39040 | 0.37280 | 0.16070 | 0.3693 | 0.09618 |\n",
       "| 46 |  TRUE | 18.65 | 17.60 | 123.70 | 1076.0 | 0.10990 | 0.16860 | 0.19740 | 0.10090 | 0.1907 | ⋯ | 22.82 | 21.32 | 150.60 | 1567.0 | 0.16790 | 0.50900 | 0.73450 | 0.23780 | 0.3799 | 0.09185 |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 525 | FALSE |  9.847 | 15.68 |  63.00 |  293.2 | 0.09492 | 0.08419 | 0.02330 | 0.024160 | 0.1387 | ⋯ | 11.240 | 22.99 |  74.32 |  376.5 | 0.14190 | 0.22430 | 0.08434 | 0.06528 | 0.2502 | 0.09209 |\n",
       "| 526 | FALSE |  8.571 | 13.10 |  54.53 |  221.3 | 0.10360 | 0.07632 | 0.02565 | 0.015100 | 0.1678 | ⋯ |  9.473 | 18.45 |  63.30 |  275.6 | 0.16410 | 0.22350 | 0.17540 | 0.08512 | 0.2983 | 0.10490 |\n",
       "| 528 | FALSE | 12.340 | 12.27 |  78.94 |  468.5 | 0.09003 | 0.06307 | 0.02958 | 0.026470 | 0.1689 | ⋯ | 13.610 | 19.27 |  87.22 |  564.9 | 0.12920 | 0.20740 | 0.17910 | 0.10700 | 0.3110 | 0.07592 |\n",
       "| 529 | FALSE | 13.940 | 13.17 |  90.31 |  594.2 | 0.12480 | 0.09755 | 0.10100 | 0.066150 | 0.1976 | ⋯ | 14.620 | 15.38 |  94.52 |  653.3 | 0.13940 | 0.13640 | 0.15590 | 0.10150 | 0.2160 | 0.07253 |\n",
       "| 531 | FALSE | 11.750 | 17.56 |  75.89 |  422.9 | 0.10730 | 0.09713 | 0.05282 | 0.044400 | 0.1598 | ⋯ | 13.500 | 27.98 |  88.52 |  552.3 | 0.13490 | 0.18540 | 0.13660 | 0.10100 | 0.2478 | 0.07757 |\n",
       "| 532 | FALSE | 11.670 | 20.02 |  75.21 |  416.2 | 0.10160 | 0.09453 | 0.04200 | 0.021570 | 0.1859 | ⋯ | 13.350 | 28.81 |  87.00 |  550.6 | 0.15500 | 0.29640 | 0.27580 | 0.08120 | 0.3206 | 0.08950 |\n",
       "| 533 | FALSE | 13.680 | 16.33 |  87.76 |  575.5 | 0.09277 | 0.07255 | 0.01752 | 0.018800 | 0.1631 | ⋯ | 15.850 | 20.20 | 101.60 |  773.4 | 0.12640 | 0.15640 | 0.12060 | 0.08704 | 0.2806 | 0.07782 |\n",
       "| 534 |  TRUE | 20.470 | 20.67 | 134.70 | 1299.0 | 0.09156 | 0.13130 | 0.15230 | 0.101500 | 0.2166 | ⋯ | 23.230 | 27.15 | 152.00 | 1645.0 | 0.10970 | 0.25340 | 0.30920 | 0.16130 | 0.3220 | 0.06386 |\n",
       "| 537 |  TRUE | 14.270 | 22.55 |  93.77 |  629.8 | 0.10380 | 0.11540 | 0.14630 | 0.061390 | 0.1926 | ⋯ | 15.290 | 34.27 | 104.30 |  728.3 | 0.13800 | 0.27330 | 0.42340 | 0.13620 | 0.2698 | 0.08351 |\n",
       "| 538 | FALSE | 11.690 | 24.44 |  76.37 |  406.4 | 0.12360 | 0.15520 | 0.04515 | 0.045310 | 0.2131 | ⋯ | 12.980 | 32.19 |  86.12 |  487.7 | 0.17680 | 0.32510 | 0.13950 | 0.13080 | 0.2803 | 0.09970 |\n",
       "| 540 | FALSE |  7.691 | 25.44 |  48.34 |  170.4 | 0.08668 | 0.11990 | 0.09252 | 0.013640 | 0.2037 | ⋯ |  8.678 | 31.89 |  54.49 |  223.6 | 0.15960 | 0.30640 | 0.33930 | 0.05000 | 0.2790 | 0.10660 |\n",
       "| 541 | FALSE | 11.540 | 14.44 |  74.65 |  402.9 | 0.09984 | 0.11200 | 0.06737 | 0.025940 | 0.1818 | ⋯ | 12.260 | 19.68 |  78.78 |  457.8 | 0.13450 | 0.21180 | 0.17970 | 0.06918 | 0.2329 | 0.08134 |\n",
       "| 542 | FALSE | 14.470 | 24.99 |  95.81 |  656.4 | 0.08837 | 0.12300 | 0.10090 | 0.038900 | 0.1872 | ⋯ | 16.220 | 31.73 | 113.50 |  808.9 | 0.13400 | 0.42020 | 0.40400 | 0.12050 | 0.3187 | 0.10230 |\n",
       "| 545 | FALSE | 13.870 | 20.70 |  89.77 |  584.8 | 0.09578 | 0.10180 | 0.03688 | 0.023690 | 0.1620 | ⋯ | 15.050 | 24.75 |  99.17 |  688.6 | 0.12640 | 0.20370 | 0.13770 | 0.06845 | 0.2249 | 0.08492 |\n",
       "| 547 | FALSE | 10.320 | 16.35 |  65.31 |  324.9 | 0.09434 | 0.04994 | 0.01012 | 0.005495 | 0.1885 | ⋯ | 11.250 | 21.77 |  71.12 |  384.9 | 0.12850 | 0.08842 | 0.04384 | 0.02381 | 0.2681 | 0.07399 |\n",
       "| 548 | FALSE | 10.260 | 16.58 |  65.85 |  320.8 | 0.08877 | 0.08066 | 0.04358 | 0.024380 | 0.1669 | ⋯ | 10.830 | 22.04 |  71.08 |  357.4 | 0.14610 | 0.22460 | 0.17830 | 0.08333 | 0.2691 | 0.09479 |\n",
       "| 549 | FALSE |  9.683 | 19.34 |  61.05 |  285.7 | 0.08491 | 0.05030 | 0.02337 | 0.009615 | 0.1580 | ⋯ | 10.930 | 25.59 |  69.10 |  364.2 | 0.11990 | 0.09546 | 0.09350 | 0.03846 | 0.2552 | 0.07920 |\n",
       "| 551 | FALSE | 10.860 | 21.48 |  68.51 |  360.5 | 0.07431 | 0.04227 | 0.00000 | 0.000000 | 0.1661 | ⋯ | 11.660 | 24.77 |  74.08 |  412.3 | 0.10010 | 0.07348 | 0.00000 | 0.00000 | 0.2458 | 0.06592 |\n",
       "| 553 | FALSE | 12.770 | 29.43 |  81.35 |  507.9 | 0.08276 | 0.04234 | 0.01997 | 0.014990 | 0.1539 | ⋯ | 13.870 | 36.00 |  88.10 |  594.7 | 0.12340 | 0.10640 | 0.08653 | 0.06498 | 0.2407 | 0.06484 |\n",
       "| 554 | FALSE |  9.333 | 21.94 |  59.01 |  264.0 | 0.09240 | 0.05605 | 0.03996 | 0.012820 | 0.1692 | ⋯ |  9.845 | 25.05 |  62.86 |  295.8 | 0.11030 | 0.08298 | 0.07993 | 0.02564 | 0.2435 | 0.07393 |\n",
       "| 556 | FALSE | 10.290 | 27.61 |  65.67 |  321.4 | 0.09030 | 0.07658 | 0.05999 | 0.027380 | 0.1593 | ⋯ | 10.840 | 34.91 |  69.57 |  357.6 | 0.13840 | 0.17100 | 0.20000 | 0.09127 | 0.2226 | 0.08283 |\n",
       "| 558 | FALSE |  9.423 | 27.88 |  59.26 |  271.3 | 0.08123 | 0.04971 | 0.00000 | 0.000000 | 0.1742 | ⋯ | 10.490 | 34.24 |  66.50 |  330.6 | 0.10730 | 0.07158 | 0.00000 | 0.00000 | 0.2475 | 0.06969 |\n",
       "| 561 | FALSE | 14.050 | 27.15 |  91.38 |  600.4 | 0.09929 | 0.11260 | 0.04462 | 0.043040 | 0.1537 | ⋯ | 15.300 | 33.17 | 100.20 |  706.7 | 0.12410 | 0.22640 | 0.13260 | 0.10480 | 0.2250 | 0.08321 |\n",
       "| 562 | FALSE | 11.200 | 29.37 |  70.67 |  386.0 | 0.07449 | 0.03558 | 0.00000 | 0.000000 | 0.1060 | ⋯ | 11.920 | 38.30 |  75.19 |  439.6 | 0.09267 | 0.05494 | 0.00000 | 0.00000 | 0.1566 | 0.05905 |\n",
       "| 563 |  TRUE | 15.220 | 30.62 | 103.40 |  716.9 | 0.10480 | 0.20870 | 0.25500 | 0.094290 | 0.2128 | ⋯ | 17.520 | 42.79 | 128.70 |  915.0 | 0.14170 | 0.79170 | 1.17000 | 0.23560 | 0.4089 | 0.14090 |\n",
       "| 564 |  TRUE | 20.920 | 25.09 | 143.00 | 1347.0 | 0.10990 | 0.22360 | 0.31740 | 0.147400 | 0.2149 | ⋯ | 24.290 | 29.41 | 179.10 | 1819.0 | 0.14070 | 0.41860 | 0.65990 | 0.25420 | 0.2929 | 0.09873 |\n",
       "| 565 |  TRUE | 21.560 | 22.39 | 142.00 | 1479.0 | 0.11100 | 0.11590 | 0.24390 | 0.138900 | 0.1726 | ⋯ | 25.450 | 26.40 | 166.10 | 2027.0 | 0.14100 | 0.21130 | 0.41070 | 0.22160 | 0.2060 | 0.07115 |\n",
       "| 567 |  TRUE | 16.600 | 28.08 | 108.30 |  858.1 | 0.08455 | 0.10230 | 0.09251 | 0.053020 | 0.1590 | ⋯ | 18.980 | 34.12 | 126.70 | 1124.0 | 0.11390 | 0.30940 | 0.34030 | 0.14180 | 0.2218 | 0.07820 |\n",
       "| 568 |  TRUE | 20.600 | 29.33 | 140.10 | 1265.0 | 0.11780 | 0.27700 | 0.35140 | 0.152000 | 0.2397 | ⋯ | 25.740 | 39.42 | 184.60 | 1821.0 | 0.16500 | 0.86810 | 0.93870 | 0.26500 | 0.4087 | 0.12400 |\n",
       "| 569 | FALSE |  7.760 | 24.54 |  47.92 |  181.0 | 0.05263 | 0.04362 | 0.00000 | 0.000000 | 0.1587 | ⋯ |  9.456 | 30.37 |  59.16 |  268.6 | 0.08996 | 0.06444 | 0.00000 | 0.00000 | 0.2871 | 0.07039 |\n",
       "\n"
      ],
      "text/plain": [
       "    train_data_y radius_mean texture_mean perimeter_mean area_mean\n",
       "2    TRUE        20.57       17.77        132.90         1326.0   \n",
       "3    TRUE        19.69       21.25        130.00         1203.0   \n",
       "4    TRUE        11.42       20.38         77.58          386.1   \n",
       "10   TRUE        12.46       24.04         83.97          475.9   \n",
       "11   TRUE        16.02       23.24        102.70          797.8   \n",
       "13   TRUE        19.17       24.80        132.40         1123.0   \n",
       "14   TRUE        15.85       23.95        103.70          782.7   \n",
       "15   TRUE        13.73       22.61         93.60          578.3   \n",
       "16   TRUE        14.54       27.54         96.73          658.8   \n",
       "17   TRUE        14.68       20.13         94.74          684.5   \n",
       "18   TRUE        16.13       20.68        108.10          798.8   \n",
       "19   TRUE        19.81       22.15        130.00         1260.0   \n",
       "23   TRUE        15.34       14.26        102.50          704.4   \n",
       "24   TRUE        21.16       23.04        137.20         1404.0   \n",
       "27   TRUE        14.58       21.53         97.41          644.8   \n",
       "28   TRUE        18.61       20.25        122.10         1094.0   \n",
       "29   TRUE        15.30       25.27        102.40          732.4   \n",
       "32   TRUE        11.84       18.70         77.93          440.6   \n",
       "33   TRUE        17.02       23.98        112.80          899.3   \n",
       "34   TRUE        19.27       26.47        127.90         1162.0   \n",
       "36   TRUE        16.74       21.59        110.10          869.5   \n",
       "37   TRUE        14.25       21.72         93.63          633.0   \n",
       "38  FALSE        13.03       18.42         82.61          523.8   \n",
       "39   TRUE        14.99       25.20         95.54          698.8   \n",
       "40   TRUE        13.48       20.82         88.40          559.2   \n",
       "42   TRUE        10.95       21.35         71.90          371.1   \n",
       "43   TRUE        19.07       24.81        128.30         1104.0   \n",
       "44   TRUE        13.28       20.28         87.32          545.2   \n",
       "45   TRUE        13.17       21.81         85.42          531.5   \n",
       "46   TRUE        18.65       17.60        123.70         1076.0   \n",
       "⋮   ⋮            ⋮           ⋮            ⋮              ⋮        \n",
       "525 FALSE         9.847      15.68         63.00          293.2   \n",
       "526 FALSE         8.571      13.10         54.53          221.3   \n",
       "528 FALSE        12.340      12.27         78.94          468.5   \n",
       "529 FALSE        13.940      13.17         90.31          594.2   \n",
       "531 FALSE        11.750      17.56         75.89          422.9   \n",
       "532 FALSE        11.670      20.02         75.21          416.2   \n",
       "533 FALSE        13.680      16.33         87.76          575.5   \n",
       "534  TRUE        20.470      20.67        134.70         1299.0   \n",
       "537  TRUE        14.270      22.55         93.77          629.8   \n",
       "538 FALSE        11.690      24.44         76.37          406.4   \n",
       "540 FALSE         7.691      25.44         48.34          170.4   \n",
       "541 FALSE        11.540      14.44         74.65          402.9   \n",
       "542 FALSE        14.470      24.99         95.81          656.4   \n",
       "545 FALSE        13.870      20.70         89.77          584.8   \n",
       "547 FALSE        10.320      16.35         65.31          324.9   \n",
       "548 FALSE        10.260      16.58         65.85          320.8   \n",
       "549 FALSE         9.683      19.34         61.05          285.7   \n",
       "551 FALSE        10.860      21.48         68.51          360.5   \n",
       "553 FALSE        12.770      29.43         81.35          507.9   \n",
       "554 FALSE         9.333      21.94         59.01          264.0   \n",
       "556 FALSE        10.290      27.61         65.67          321.4   \n",
       "558 FALSE         9.423      27.88         59.26          271.3   \n",
       "561 FALSE        14.050      27.15         91.38          600.4   \n",
       "562 FALSE        11.200      29.37         70.67          386.0   \n",
       "563  TRUE        15.220      30.62        103.40          716.9   \n",
       "564  TRUE        20.920      25.09        143.00         1347.0   \n",
       "565  TRUE        21.560      22.39        142.00         1479.0   \n",
       "567  TRUE        16.600      28.08        108.30          858.1   \n",
       "568  TRUE        20.600      29.33        140.10         1265.0   \n",
       "569 FALSE         7.760      24.54         47.92          181.0   \n",
       "    smoothness_mean compactness_mean concavity_mean concave.points_mean\n",
       "2   0.08474         0.07864          0.08690        0.07017            \n",
       "3   0.10960         0.15990          0.19740        0.12790            \n",
       "4   0.14250         0.28390          0.24140        0.10520            \n",
       "10  0.11860         0.23960          0.22730        0.08543            \n",
       "11  0.08206         0.06669          0.03299        0.03323            \n",
       "13  0.09740         0.24580          0.20650        0.11180            \n",
       "14  0.08401         0.10020          0.09938        0.05364            \n",
       "15  0.11310         0.22930          0.21280        0.08025            \n",
       "16  0.11390         0.15950          0.16390        0.07364            \n",
       "17  0.09867         0.07200          0.07395        0.05259            \n",
       "18  0.11700         0.20220          0.17220        0.10280            \n",
       "19  0.09831         0.10270          0.14790        0.09498            \n",
       "23  0.10730         0.21350          0.20770        0.09756            \n",
       "24  0.09428         0.10220          0.10970        0.08632            \n",
       "27  0.10540         0.18680          0.14250        0.08783            \n",
       "28  0.09440         0.10660          0.14900        0.07731            \n",
       "29  0.10820         0.16970          0.16830        0.08751            \n",
       "32  0.11090         0.15160          0.12180        0.05182            \n",
       "33  0.11970         0.14960          0.24170        0.12030            \n",
       "34  0.09401         0.17190          0.16570        0.07593            \n",
       "36  0.09610         0.13360          0.13480        0.06018            \n",
       "37  0.09823         0.10980          0.13190        0.05598            \n",
       "38  0.08983         0.03766          0.02562        0.02923            \n",
       "39  0.09387         0.05131          0.02398        0.02899            \n",
       "40  0.10160         0.12550          0.10630        0.05439            \n",
       "42  0.12270         0.12180          0.10440        0.05669            \n",
       "43  0.09081         0.21900          0.21070        0.09961            \n",
       "44  0.10410         0.14360          0.09847        0.06158            \n",
       "45  0.09714         0.10470          0.08259        0.05252            \n",
       "46  0.10990         0.16860          0.19740        0.10090            \n",
       "⋮   ⋮               ⋮                ⋮              ⋮                  \n",
       "525 0.09492         0.08419          0.02330        0.024160           \n",
       "526 0.10360         0.07632          0.02565        0.015100           \n",
       "528 0.09003         0.06307          0.02958        0.026470           \n",
       "529 0.12480         0.09755          0.10100        0.066150           \n",
       "531 0.10730         0.09713          0.05282        0.044400           \n",
       "532 0.10160         0.09453          0.04200        0.021570           \n",
       "533 0.09277         0.07255          0.01752        0.018800           \n",
       "534 0.09156         0.13130          0.15230        0.101500           \n",
       "537 0.10380         0.11540          0.14630        0.061390           \n",
       "538 0.12360         0.15520          0.04515        0.045310           \n",
       "540 0.08668         0.11990          0.09252        0.013640           \n",
       "541 0.09984         0.11200          0.06737        0.025940           \n",
       "542 0.08837         0.12300          0.10090        0.038900           \n",
       "545 0.09578         0.10180          0.03688        0.023690           \n",
       "547 0.09434         0.04994          0.01012        0.005495           \n",
       "548 0.08877         0.08066          0.04358        0.024380           \n",
       "549 0.08491         0.05030          0.02337        0.009615           \n",
       "551 0.07431         0.04227          0.00000        0.000000           \n",
       "553 0.08276         0.04234          0.01997        0.014990           \n",
       "554 0.09240         0.05605          0.03996        0.012820           \n",
       "556 0.09030         0.07658          0.05999        0.027380           \n",
       "558 0.08123         0.04971          0.00000        0.000000           \n",
       "561 0.09929         0.11260          0.04462        0.043040           \n",
       "562 0.07449         0.03558          0.00000        0.000000           \n",
       "563 0.10480         0.20870          0.25500        0.094290           \n",
       "564 0.10990         0.22360          0.31740        0.147400           \n",
       "565 0.11100         0.11590          0.24390        0.138900           \n",
       "567 0.08455         0.10230          0.09251        0.053020           \n",
       "568 0.11780         0.27700          0.35140        0.152000           \n",
       "569 0.05263         0.04362          0.00000        0.000000           \n",
       "    symmetry_mean ⋯ radius_worst texture_worst perimeter_worst area_worst\n",
       "2   0.1812        ⋯ 24.99        23.41         158.80          1956.0    \n",
       "3   0.2069        ⋯ 23.57        25.53         152.50          1709.0    \n",
       "4   0.2597        ⋯ 14.91        26.50          98.87           567.7    \n",
       "10  0.2030        ⋯ 15.09        40.68          97.65           711.4    \n",
       "11  0.1528        ⋯ 19.19        33.88         123.80          1150.0    \n",
       "13  0.2397        ⋯ 20.96        29.94         151.70          1332.0    \n",
       "14  0.1847        ⋯ 16.84        27.66         112.00           876.5    \n",
       "15  0.2069        ⋯ 15.03        32.01         108.80           697.7    \n",
       "16  0.2303        ⋯ 17.46        37.13         124.10           943.2    \n",
       "17  0.1586        ⋯ 19.07        30.88         123.40          1138.0    \n",
       "18  0.2164        ⋯ 20.96        31.48         136.80          1315.0    \n",
       "19  0.1582        ⋯ 27.32        30.88         186.80          2398.0    \n",
       "23  0.2521        ⋯ 18.07        19.08         125.10           980.9    \n",
       "24  0.1769        ⋯ 29.17        35.59         188.00          2615.0    \n",
       "27  0.2252        ⋯ 17.62        33.21         122.40           896.9    \n",
       "28  0.1697        ⋯ 21.31        27.26         139.90          1403.0    \n",
       "29  0.1926        ⋯ 20.27        36.71         149.30          1269.0    \n",
       "32  0.2301        ⋯ 16.82        28.12         119.40           888.7    \n",
       "33  0.2248        ⋯ 20.88        32.09         136.10          1344.0    \n",
       "34  0.1853        ⋯ 24.15        30.90         161.40          1813.0    \n",
       "36  0.1896        ⋯ 20.01        29.02         133.50          1229.0    \n",
       "37  0.1885        ⋯ 15.89        30.36         116.20           799.6    \n",
       "38  0.1467        ⋯ 13.30        22.81          84.46           545.9    \n",
       "39  0.1565        ⋯ 14.99        25.20          95.54           698.8    \n",
       "40  0.1720        ⋯ 15.53        26.02         107.30           740.4    \n",
       "42  0.1895        ⋯ 12.84        35.34          87.22           514.0    \n",
       "43  0.2310        ⋯ 24.09        33.17         177.40          1651.0    \n",
       "44  0.1974        ⋯ 17.38        28.00         113.10           907.2    \n",
       "45  0.1746        ⋯ 16.23        29.89         105.50           740.7    \n",
       "46  0.1907        ⋯ 22.82        21.32         150.60          1567.0    \n",
       "⋮   ⋮             ⋱ ⋮            ⋮             ⋮               ⋮         \n",
       "525 0.1387        ⋯ 11.240       22.99          74.32           376.5    \n",
       "526 0.1678        ⋯  9.473       18.45          63.30           275.6    \n",
       "528 0.1689        ⋯ 13.610       19.27          87.22           564.9    \n",
       "529 0.1976        ⋯ 14.620       15.38          94.52           653.3    \n",
       "531 0.1598        ⋯ 13.500       27.98          88.52           552.3    \n",
       "532 0.1859        ⋯ 13.350       28.81          87.00           550.6    \n",
       "533 0.1631        ⋯ 15.850       20.20         101.60           773.4    \n",
       "534 0.2166        ⋯ 23.230       27.15         152.00          1645.0    \n",
       "537 0.1926        ⋯ 15.290       34.27         104.30           728.3    \n",
       "538 0.2131        ⋯ 12.980       32.19          86.12           487.7    \n",
       "540 0.2037        ⋯  8.678       31.89          54.49           223.6    \n",
       "541 0.1818        ⋯ 12.260       19.68          78.78           457.8    \n",
       "542 0.1872        ⋯ 16.220       31.73         113.50           808.9    \n",
       "545 0.1620        ⋯ 15.050       24.75          99.17           688.6    \n",
       "547 0.1885        ⋯ 11.250       21.77          71.12           384.9    \n",
       "548 0.1669        ⋯ 10.830       22.04          71.08           357.4    \n",
       "549 0.1580        ⋯ 10.930       25.59          69.10           364.2    \n",
       "551 0.1661        ⋯ 11.660       24.77          74.08           412.3    \n",
       "553 0.1539        ⋯ 13.870       36.00          88.10           594.7    \n",
       "554 0.1692        ⋯  9.845       25.05          62.86           295.8    \n",
       "556 0.1593        ⋯ 10.840       34.91          69.57           357.6    \n",
       "558 0.1742        ⋯ 10.490       34.24          66.50           330.6    \n",
       "561 0.1537        ⋯ 15.300       33.17         100.20           706.7    \n",
       "562 0.1060        ⋯ 11.920       38.30          75.19           439.6    \n",
       "563 0.2128        ⋯ 17.520       42.79         128.70           915.0    \n",
       "564 0.2149        ⋯ 24.290       29.41         179.10          1819.0    \n",
       "565 0.1726        ⋯ 25.450       26.40         166.10          2027.0    \n",
       "567 0.1590        ⋯ 18.980       34.12         126.70          1124.0    \n",
       "568 0.2397        ⋯ 25.740       39.42         184.60          1821.0    \n",
       "569 0.1587        ⋯  9.456       30.37          59.16           268.6    \n",
       "    smoothness_worst compactness_worst concavity_worst concave.points_worst\n",
       "2   0.12380          0.18660           0.24160         0.18600             \n",
       "3   0.14440          0.42450           0.45040         0.24300             \n",
       "4   0.20980          0.86630           0.68690         0.25750             \n",
       "10  0.18530          1.05800           1.10500         0.22100             \n",
       "11  0.11810          0.15510           0.14590         0.09975             \n",
       "13  0.10370          0.39030           0.36390         0.17670             \n",
       "14  0.11310          0.19240           0.23220         0.11190             \n",
       "15  0.16510          0.77250           0.69430         0.22080             \n",
       "16  0.16780          0.65770           0.70260         0.17120             \n",
       "17  0.14640          0.18710           0.29140         0.16090             \n",
       "18  0.17890          0.42330           0.47840         0.20730             \n",
       "19  0.15120          0.31500           0.53720         0.23880             \n",
       "23  0.13900          0.59540           0.63050         0.23930             \n",
       "24  0.14010          0.26000           0.31550         0.20090             \n",
       "27  0.15250          0.66430           0.55390         0.27010             \n",
       "28  0.13380          0.21170           0.34460         0.14900             \n",
       "29  0.16410          0.61100           0.63350         0.20240             \n",
       "32  0.16370          0.57750           0.69560         0.15460             \n",
       "33  0.16340          0.35590           0.55880         0.18470             \n",
       "34  0.15090          0.65900           0.60910         0.17850             \n",
       "36  0.15630          0.38350           0.54090         0.18130             \n",
       "37  0.14460          0.42380           0.51860         0.14470             \n",
       "38  0.09701          0.04619           0.04833         0.05013             \n",
       "39  0.09387          0.05131           0.02398         0.02899             \n",
       "40  0.16100          0.42250           0.50300         0.22580             \n",
       "42  0.19090          0.26980           0.40230         0.14240             \n",
       "43  0.12470          0.74440           0.72420         0.24930             \n",
       "44  0.15300          0.37240           0.36640         0.14920             \n",
       "45  0.15030          0.39040           0.37280         0.16070             \n",
       "46  0.16790          0.50900           0.73450         0.23780             \n",
       "⋮   ⋮                ⋮                 ⋮               ⋮                   \n",
       "525 0.14190          0.22430           0.08434         0.06528             \n",
       "526 0.16410          0.22350           0.17540         0.08512             \n",
       "528 0.12920          0.20740           0.17910         0.10700             \n",
       "529 0.13940          0.13640           0.15590         0.10150             \n",
       "531 0.13490          0.18540           0.13660         0.10100             \n",
       "532 0.15500          0.29640           0.27580         0.08120             \n",
       "533 0.12640          0.15640           0.12060         0.08704             \n",
       "534 0.10970          0.25340           0.30920         0.16130             \n",
       "537 0.13800          0.27330           0.42340         0.13620             \n",
       "538 0.17680          0.32510           0.13950         0.13080             \n",
       "540 0.15960          0.30640           0.33930         0.05000             \n",
       "541 0.13450          0.21180           0.17970         0.06918             \n",
       "542 0.13400          0.42020           0.40400         0.12050             \n",
       "545 0.12640          0.20370           0.13770         0.06845             \n",
       "547 0.12850          0.08842           0.04384         0.02381             \n",
       "548 0.14610          0.22460           0.17830         0.08333             \n",
       "549 0.11990          0.09546           0.09350         0.03846             \n",
       "551 0.10010          0.07348           0.00000         0.00000             \n",
       "553 0.12340          0.10640           0.08653         0.06498             \n",
       "554 0.11030          0.08298           0.07993         0.02564             \n",
       "556 0.13840          0.17100           0.20000         0.09127             \n",
       "558 0.10730          0.07158           0.00000         0.00000             \n",
       "561 0.12410          0.22640           0.13260         0.10480             \n",
       "562 0.09267          0.05494           0.00000         0.00000             \n",
       "563 0.14170          0.79170           1.17000         0.23560             \n",
       "564 0.14070          0.41860           0.65990         0.25420             \n",
       "565 0.14100          0.21130           0.41070         0.22160             \n",
       "567 0.11390          0.30940           0.34030         0.14180             \n",
       "568 0.16500          0.86810           0.93870         0.26500             \n",
       "569 0.08996          0.06444           0.00000         0.00000             \n",
       "    symmetry_worst fractal_dimension_worst\n",
       "2   0.2750         0.08902                \n",
       "3   0.3613         0.08758                \n",
       "4   0.6638         0.17300                \n",
       "10  0.4366         0.20750                \n",
       "11  0.2948         0.08452                \n",
       "13  0.3176         0.10230                \n",
       "14  0.2809         0.06287                \n",
       "15  0.3596         0.14310                \n",
       "16  0.4218         0.13410                \n",
       "17  0.3029         0.08216                \n",
       "18  0.3706         0.11420                \n",
       "19  0.2768         0.07615                \n",
       "23  0.4667         0.09946                \n",
       "24  0.2822         0.07526                \n",
       "27  0.4264         0.12750                \n",
       "28  0.2341         0.07421                \n",
       "29  0.4027         0.09876                \n",
       "32  0.4761         0.14020                \n",
       "33  0.3530         0.08482                \n",
       "34  0.3672         0.11230                \n",
       "36  0.4863         0.08633                \n",
       "37  0.3591         0.10140                \n",
       "38  0.1987         0.06169                \n",
       "39  0.1565         0.05504                \n",
       "40  0.2807         0.10710                \n",
       "42  0.2964         0.09606                \n",
       "43  0.4670         0.10380                \n",
       "44  0.3739         0.10270                \n",
       "45  0.3693         0.09618                \n",
       "46  0.3799         0.09185                \n",
       "⋮   ⋮              ⋮                      \n",
       "525 0.2502         0.09209                \n",
       "526 0.2983         0.10490                \n",
       "528 0.3110         0.07592                \n",
       "529 0.2160         0.07253                \n",
       "531 0.2478         0.07757                \n",
       "532 0.3206         0.08950                \n",
       "533 0.2806         0.07782                \n",
       "534 0.3220         0.06386                \n",
       "537 0.2698         0.08351                \n",
       "538 0.2803         0.09970                \n",
       "540 0.2790         0.10660                \n",
       "541 0.2329         0.08134                \n",
       "542 0.3187         0.10230                \n",
       "545 0.2249         0.08492                \n",
       "547 0.2681         0.07399                \n",
       "548 0.2691         0.09479                \n",
       "549 0.2552         0.07920                \n",
       "551 0.2458         0.06592                \n",
       "553 0.2407         0.06484                \n",
       "554 0.2435         0.07393                \n",
       "556 0.2226         0.08283                \n",
       "558 0.2475         0.06969                \n",
       "561 0.2250         0.08321                \n",
       "562 0.1566         0.05905                \n",
       "563 0.4089         0.14090                \n",
       "564 0.2929         0.09873                \n",
       "565 0.2060         0.07115                \n",
       "567 0.2218         0.07820                \n",
       "568 0.4087         0.12400                \n",
       "569 0.2871         0.07039                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=data.frame(train_data_y, train_data_X)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Networks_4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logist <- function(x){\n",
    "  y = 1 / (1 + exp(-x))\n",
    "}\n",
    "p1 <- ggplot(tibble())\n",
    "p1 + stat_function(aes(-5:5), fun = logist) + xlab(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pngwave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hidden: 50, 5    thresh: 0.01    rep: 1/1    steps:    1000\tmin thresh: 0.441411830205445\n",
      "                                                       2000\tmin thresh: 0.425272083837252\n",
      "                                                       3000\tmin thresh: 0.190913536787886\n",
      "                                                       4000\tmin thresh: 0.107968214748922\n",
      "                                                       5000\tmin thresh: 0.0721652926770795\n",
      "                                                       6000\tmin thresh: 0.0526476922378954\n",
      "                                                       7000\tmin thresh: 0.0406546209498755\n",
      "                                                       8000\tmin thresh: 0.0325923483641238\n",
      "                                                       9000\tmin thresh: 0.0268387458557384\n",
      "                                                      10000\tmin thresh: 0.0225491157715334\n",
      "                                                      11000\tmin thresh: 0.0192422554155631\n",
      "                                                      12000\tmin thresh: 0.0166249643265973\n",
      "                                                      13000\tmin thresh: 0.0145089496222991\n",
      "                                                      14000\tmin thresh: 0.0127680696678832\n",
      "                                                      15000\tmin thresh: 0.0113148506500245\n",
      "                                                      16000\tmin thresh: 0.010086923328604\n",
      "                                                      16078\terror: 24.15072\ttime: 34.26 secs\n"
     ]
    }
   ],
   "source": [
    "set.seed(42)\n",
    "nn=neuralnet(train_data_y ~ ., \n",
    "             data=train, \n",
    "             hidden=c(50,5), \n",
    "             lifesign='full',\n",
    "             algorithm='backprop',\n",
    "             learningrate=0.001,\n",
    "             threshold=0.01,\n",
    "             rep=1,\n",
    "             linear.output=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn)\n",
    "# plot(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 1814 × 1 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>error</th><td> 2.415072e+01</td></tr>\n",
       "\t<tr><th scope=row>reached.threshold</th><td> 9.999206e-03</td></tr>\n",
       "\t<tr><th scope=row>steps</th><td> 1.607800e+04</td></tr>\n",
       "\t<tr><th scope=row>Intercept.to.1layhid1</th><td> 1.370958e+00</td></tr>\n",
       "\t<tr><th scope=row>radius_mean.to.1layhid1</th><td>-5.646982e-01</td></tr>\n",
       "\t<tr><th scope=row>texture_mean.to.1layhid1</th><td> 3.631284e-01</td></tr>\n",
       "\t<tr><th scope=row>perimeter_mean.to.1layhid1</th><td> 6.328626e-01</td></tr>\n",
       "\t<tr><th scope=row>area_mean.to.1layhid1</th><td> 4.042683e-01</td></tr>\n",
       "\t<tr><th scope=row>smoothness_mean.to.1layhid1</th><td>-1.061245e-01</td></tr>\n",
       "\t<tr><th scope=row>compactness_mean.to.1layhid1</th><td> 1.511522e+00</td></tr>\n",
       "\t<tr><th scope=row>concavity_mean.to.1layhid1</th><td>-9.465904e-02</td></tr>\n",
       "\t<tr><th scope=row>concave.points_mean.to.1layhid1</th><td> 2.018424e+00</td></tr>\n",
       "\t<tr><th scope=row>symmetry_mean.to.1layhid1</th><td>-6.271410e-02</td></tr>\n",
       "\t<tr><th scope=row>fractal_dimension_mean.to.1layhid1</th><td> 1.304870e+00</td></tr>\n",
       "\t<tr><th scope=row>radius_se.to.1layhid1</th><td> 2.286645e+00</td></tr>\n",
       "\t<tr><th scope=row>texture_se.to.1layhid1</th><td>-1.388861e+00</td></tr>\n",
       "\t<tr><th scope=row>perimeter_se.to.1layhid1</th><td>-2.787888e-01</td></tr>\n",
       "\t<tr><th scope=row>area_se.to.1layhid1</th><td>-1.333213e-01</td></tr>\n",
       "\t<tr><th scope=row>smoothness_se.to.1layhid1</th><td> 6.359504e-01</td></tr>\n",
       "\t<tr><th scope=row>compactness_se.to.1layhid1</th><td>-2.842529e-01</td></tr>\n",
       "\t<tr><th scope=row>concavity_se.to.1layhid1</th><td>-2.656455e+00</td></tr>\n",
       "\t<tr><th scope=row>concave.points_se.to.1layhid1</th><td>-2.440467e+00</td></tr>\n",
       "\t<tr><th scope=row>symmetry_se.to.1layhid1</th><td> 1.320113e+00</td></tr>\n",
       "\t<tr><th scope=row>fractal_dimension_se.to.1layhid1</th><td>-3.066386e-01</td></tr>\n",
       "\t<tr><th scope=row>radius_worst.to.1layhid1</th><td>-1.781308e+00</td></tr>\n",
       "\t<tr><th scope=row>texture_worst.to.1layhid1</th><td>-1.719174e-01</td></tr>\n",
       "\t<tr><th scope=row>perimeter_worst.to.1layhid1</th><td> 1.214675e+00</td></tr>\n",
       "\t<tr><th scope=row>area_worst.to.1layhid1</th><td> 1.895193e+00</td></tr>\n",
       "\t<tr><th scope=row>smoothness_worst.to.1layhid1</th><td>-4.304691e-01</td></tr>\n",
       "\t<tr><th scope=row>compactness_worst.to.1layhid1</th><td>-2.572694e-01</td></tr>\n",
       "\t<tr><th scope=row>⋮</th><td>⋮</td></tr>\n",
       "\t<tr><th scope=row>1layhid27.to.2layhid5</th><td> 1.3527838210</td></tr>\n",
       "\t<tr><th scope=row>1layhid28.to.2layhid5</th><td> 1.9496417349</td></tr>\n",
       "\t<tr><th scope=row>1layhid29.to.2layhid5</th><td>-0.1740878757</td></tr>\n",
       "\t<tr><th scope=row>1layhid30.to.2layhid5</th><td>-1.4342353538</td></tr>\n",
       "\t<tr><th scope=row>1layhid31.to.2layhid5</th><td>-1.0846150093</td></tr>\n",
       "\t<tr><th scope=row>1layhid32.to.2layhid5</th><td>-0.7720077560</td></tr>\n",
       "\t<tr><th scope=row>1layhid33.to.2layhid5</th><td>-0.4265097047</td></tr>\n",
       "\t<tr><th scope=row>1layhid34.to.2layhid5</th><td>-1.2109975554</td></tr>\n",
       "\t<tr><th scope=row>1layhid35.to.2layhid5</th><td> 0.5977929978</td></tr>\n",
       "\t<tr><th scope=row>1layhid36.to.2layhid5</th><td> 1.1984208022</td></tr>\n",
       "\t<tr><th scope=row>1layhid37.to.2layhid5</th><td>-0.4432986256</td></tr>\n",
       "\t<tr><th scope=row>1layhid38.to.2layhid5</th><td>-1.2658084526</td></tr>\n",
       "\t<tr><th scope=row>1layhid39.to.2layhid5</th><td>-1.8397791480</td></tr>\n",
       "\t<tr><th scope=row>1layhid40.to.2layhid5</th><td> 0.5446766251</td></tr>\n",
       "\t<tr><th scope=row>1layhid41.to.2layhid5</th><td> 0.0315226220</td></tr>\n",
       "\t<tr><th scope=row>1layhid42.to.2layhid5</th><td>-2.1303297383</td></tr>\n",
       "\t<tr><th scope=row>1layhid43.to.2layhid5</th><td> 3.7134972189</td></tr>\n",
       "\t<tr><th scope=row>1layhid44.to.2layhid5</th><td> 0.0003283772</td></tr>\n",
       "\t<tr><th scope=row>1layhid45.to.2layhid5</th><td>-0.8437783447</td></tr>\n",
       "\t<tr><th scope=row>1layhid46.to.2layhid5</th><td>-1.2213333704</td></tr>\n",
       "\t<tr><th scope=row>1layhid47.to.2layhid5</th><td>-0.4529859936</td></tr>\n",
       "\t<tr><th scope=row>1layhid48.to.2layhid5</th><td>-0.7023199313</td></tr>\n",
       "\t<tr><th scope=row>1layhid49.to.2layhid5</th><td>-0.8998553724</td></tr>\n",
       "\t<tr><th scope=row>1layhid50.to.2layhid5</th><td>-2.3030231905</td></tr>\n",
       "\t<tr><th scope=row>Intercept.to.train_data_y</th><td>-0.4765132163</td></tr>\n",
       "\t<tr><th scope=row>2layhid1.to.train_data_y</th><td> 2.4440238563</td></tr>\n",
       "\t<tr><th scope=row>2layhid2.to.train_data_y</th><td> 0.8288879272</td></tr>\n",
       "\t<tr><th scope=row>2layhid3.to.train_data_y</th><td> 1.7460664205</td></tr>\n",
       "\t<tr><th scope=row>2layhid4.to.train_data_y</th><td> 0.3461362428</td></tr>\n",
       "\t<tr><th scope=row>2layhid5.to.train_data_y</th><td>-5.0883287388</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1814 × 1 of type dbl\n",
       "\\begin{tabular}{r|l}\n",
       "\terror &  2.415072e+01\\\\\n",
       "\treached.threshold &  9.999206e-03\\\\\n",
       "\tsteps &  1.607800e+04\\\\\n",
       "\tIntercept.to.1layhid1 &  1.370958e+00\\\\\n",
       "\tradius\\_mean.to.1layhid1 & -5.646982e-01\\\\\n",
       "\ttexture\\_mean.to.1layhid1 &  3.631284e-01\\\\\n",
       "\tperimeter\\_mean.to.1layhid1 &  6.328626e-01\\\\\n",
       "\tarea\\_mean.to.1layhid1 &  4.042683e-01\\\\\n",
       "\tsmoothness\\_mean.to.1layhid1 & -1.061245e-01\\\\\n",
       "\tcompactness\\_mean.to.1layhid1 &  1.511522e+00\\\\\n",
       "\tconcavity\\_mean.to.1layhid1 & -9.465904e-02\\\\\n",
       "\tconcave.points\\_mean.to.1layhid1 &  2.018424e+00\\\\\n",
       "\tsymmetry\\_mean.to.1layhid1 & -6.271410e-02\\\\\n",
       "\tfractal\\_dimension\\_mean.to.1layhid1 &  1.304870e+00\\\\\n",
       "\tradius\\_se.to.1layhid1 &  2.286645e+00\\\\\n",
       "\ttexture\\_se.to.1layhid1 & -1.388861e+00\\\\\n",
       "\tperimeter\\_se.to.1layhid1 & -2.787888e-01\\\\\n",
       "\tarea\\_se.to.1layhid1 & -1.333213e-01\\\\\n",
       "\tsmoothness\\_se.to.1layhid1 &  6.359504e-01\\\\\n",
       "\tcompactness\\_se.to.1layhid1 & -2.842529e-01\\\\\n",
       "\tconcavity\\_se.to.1layhid1 & -2.656455e+00\\\\\n",
       "\tconcave.points\\_se.to.1layhid1 & -2.440467e+00\\\\\n",
       "\tsymmetry\\_se.to.1layhid1 &  1.320113e+00\\\\\n",
       "\tfractal\\_dimension\\_se.to.1layhid1 & -3.066386e-01\\\\\n",
       "\tradius\\_worst.to.1layhid1 & -1.781308e+00\\\\\n",
       "\ttexture\\_worst.to.1layhid1 & -1.719174e-01\\\\\n",
       "\tperimeter\\_worst.to.1layhid1 &  1.214675e+00\\\\\n",
       "\tarea\\_worst.to.1layhid1 &  1.895193e+00\\\\\n",
       "\tsmoothness\\_worst.to.1layhid1 & -4.304691e-01\\\\\n",
       "\tcompactness\\_worst.to.1layhid1 & -2.572694e-01\\\\\n",
       "\t⋮ & ⋮\\\\\n",
       "\t1layhid27.to.2layhid5 &  1.3527838210\\\\\n",
       "\t1layhid28.to.2layhid5 &  1.9496417349\\\\\n",
       "\t1layhid29.to.2layhid5 & -0.1740878757\\\\\n",
       "\t1layhid30.to.2layhid5 & -1.4342353538\\\\\n",
       "\t1layhid31.to.2layhid5 & -1.0846150093\\\\\n",
       "\t1layhid32.to.2layhid5 & -0.7720077560\\\\\n",
       "\t1layhid33.to.2layhid5 & -0.4265097047\\\\\n",
       "\t1layhid34.to.2layhid5 & -1.2109975554\\\\\n",
       "\t1layhid35.to.2layhid5 &  0.5977929978\\\\\n",
       "\t1layhid36.to.2layhid5 &  1.1984208022\\\\\n",
       "\t1layhid37.to.2layhid5 & -0.4432986256\\\\\n",
       "\t1layhid38.to.2layhid5 & -1.2658084526\\\\\n",
       "\t1layhid39.to.2layhid5 & -1.8397791480\\\\\n",
       "\t1layhid40.to.2layhid5 &  0.5446766251\\\\\n",
       "\t1layhid41.to.2layhid5 &  0.0315226220\\\\\n",
       "\t1layhid42.to.2layhid5 & -2.1303297383\\\\\n",
       "\t1layhid43.to.2layhid5 &  3.7134972189\\\\\n",
       "\t1layhid44.to.2layhid5 &  0.0003283772\\\\\n",
       "\t1layhid45.to.2layhid5 & -0.8437783447\\\\\n",
       "\t1layhid46.to.2layhid5 & -1.2213333704\\\\\n",
       "\t1layhid47.to.2layhid5 & -0.4529859936\\\\\n",
       "\t1layhid48.to.2layhid5 & -0.7023199313\\\\\n",
       "\t1layhid49.to.2layhid5 & -0.8998553724\\\\\n",
       "\t1layhid50.to.2layhid5 & -2.3030231905\\\\\n",
       "\tIntercept.to.train\\_data\\_y & -0.4765132163\\\\\n",
       "\t2layhid1.to.train\\_data\\_y &  2.4440238563\\\\\n",
       "\t2layhid2.to.train\\_data\\_y &  0.8288879272\\\\\n",
       "\t2layhid3.to.train\\_data\\_y &  1.7460664205\\\\\n",
       "\t2layhid4.to.train\\_data\\_y &  0.3461362428\\\\\n",
       "\t2layhid5.to.train\\_data\\_y & -5.0883287388\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1814 × 1 of type dbl\n",
       "\n",
       "| error |  2.415072e+01 |\n",
       "| reached.threshold |  9.999206e-03 |\n",
       "| steps |  1.607800e+04 |\n",
       "| Intercept.to.1layhid1 |  1.370958e+00 |\n",
       "| radius_mean.to.1layhid1 | -5.646982e-01 |\n",
       "| texture_mean.to.1layhid1 |  3.631284e-01 |\n",
       "| perimeter_mean.to.1layhid1 |  6.328626e-01 |\n",
       "| area_mean.to.1layhid1 |  4.042683e-01 |\n",
       "| smoothness_mean.to.1layhid1 | -1.061245e-01 |\n",
       "| compactness_mean.to.1layhid1 |  1.511522e+00 |\n",
       "| concavity_mean.to.1layhid1 | -9.465904e-02 |\n",
       "| concave.points_mean.to.1layhid1 |  2.018424e+00 |\n",
       "| symmetry_mean.to.1layhid1 | -6.271410e-02 |\n",
       "| fractal_dimension_mean.to.1layhid1 |  1.304870e+00 |\n",
       "| radius_se.to.1layhid1 |  2.286645e+00 |\n",
       "| texture_se.to.1layhid1 | -1.388861e+00 |\n",
       "| perimeter_se.to.1layhid1 | -2.787888e-01 |\n",
       "| area_se.to.1layhid1 | -1.333213e-01 |\n",
       "| smoothness_se.to.1layhid1 |  6.359504e-01 |\n",
       "| compactness_se.to.1layhid1 | -2.842529e-01 |\n",
       "| concavity_se.to.1layhid1 | -2.656455e+00 |\n",
       "| concave.points_se.to.1layhid1 | -2.440467e+00 |\n",
       "| symmetry_se.to.1layhid1 |  1.320113e+00 |\n",
       "| fractal_dimension_se.to.1layhid1 | -3.066386e-01 |\n",
       "| radius_worst.to.1layhid1 | -1.781308e+00 |\n",
       "| texture_worst.to.1layhid1 | -1.719174e-01 |\n",
       "| perimeter_worst.to.1layhid1 |  1.214675e+00 |\n",
       "| area_worst.to.1layhid1 |  1.895193e+00 |\n",
       "| smoothness_worst.to.1layhid1 | -4.304691e-01 |\n",
       "| compactness_worst.to.1layhid1 | -2.572694e-01 |\n",
       "| ⋮ | ⋮ |\n",
       "| 1layhid27.to.2layhid5 |  1.3527838210 |\n",
       "| 1layhid28.to.2layhid5 |  1.9496417349 |\n",
       "| 1layhid29.to.2layhid5 | -0.1740878757 |\n",
       "| 1layhid30.to.2layhid5 | -1.4342353538 |\n",
       "| 1layhid31.to.2layhid5 | -1.0846150093 |\n",
       "| 1layhid32.to.2layhid5 | -0.7720077560 |\n",
       "| 1layhid33.to.2layhid5 | -0.4265097047 |\n",
       "| 1layhid34.to.2layhid5 | -1.2109975554 |\n",
       "| 1layhid35.to.2layhid5 |  0.5977929978 |\n",
       "| 1layhid36.to.2layhid5 |  1.1984208022 |\n",
       "| 1layhid37.to.2layhid5 | -0.4432986256 |\n",
       "| 1layhid38.to.2layhid5 | -1.2658084526 |\n",
       "| 1layhid39.to.2layhid5 | -1.8397791480 |\n",
       "| 1layhid40.to.2layhid5 |  0.5446766251 |\n",
       "| 1layhid41.to.2layhid5 |  0.0315226220 |\n",
       "| 1layhid42.to.2layhid5 | -2.1303297383 |\n",
       "| 1layhid43.to.2layhid5 |  3.7134972189 |\n",
       "| 1layhid44.to.2layhid5 |  0.0003283772 |\n",
       "| 1layhid45.to.2layhid5 | -0.8437783447 |\n",
       "| 1layhid46.to.2layhid5 | -1.2213333704 |\n",
       "| 1layhid47.to.2layhid5 | -0.4529859936 |\n",
       "| 1layhid48.to.2layhid5 | -0.7023199313 |\n",
       "| 1layhid49.to.2layhid5 | -0.8998553724 |\n",
       "| 1layhid50.to.2layhid5 | -2.3030231905 |\n",
       "| Intercept.to.train_data_y | -0.4765132163 |\n",
       "| 2layhid1.to.train_data_y |  2.4440238563 |\n",
       "| 2layhid2.to.train_data_y |  0.8288879272 |\n",
       "| 2layhid3.to.train_data_y |  1.7460664205 |\n",
       "| 2layhid4.to.train_data_y |  0.3461362428 |\n",
       "| 2layhid5.to.train_data_y | -5.0883287388 |\n",
       "\n"
      ],
      "text/plain": [
       "                                   [,1]         \n",
       "error                               2.415072e+01\n",
       "reached.threshold                   9.999206e-03\n",
       "steps                               1.607800e+04\n",
       "Intercept.to.1layhid1               1.370958e+00\n",
       "radius_mean.to.1layhid1            -5.646982e-01\n",
       "texture_mean.to.1layhid1            3.631284e-01\n",
       "perimeter_mean.to.1layhid1          6.328626e-01\n",
       "area_mean.to.1layhid1               4.042683e-01\n",
       "smoothness_mean.to.1layhid1        -1.061245e-01\n",
       "compactness_mean.to.1layhid1        1.511522e+00\n",
       "concavity_mean.to.1layhid1         -9.465904e-02\n",
       "concave.points_mean.to.1layhid1     2.018424e+00\n",
       "symmetry_mean.to.1layhid1          -6.271410e-02\n",
       "fractal_dimension_mean.to.1layhid1  1.304870e+00\n",
       "radius_se.to.1layhid1               2.286645e+00\n",
       "texture_se.to.1layhid1             -1.388861e+00\n",
       "perimeter_se.to.1layhid1           -2.787888e-01\n",
       "area_se.to.1layhid1                -1.333213e-01\n",
       "smoothness_se.to.1layhid1           6.359504e-01\n",
       "compactness_se.to.1layhid1         -2.842529e-01\n",
       "concavity_se.to.1layhid1           -2.656455e+00\n",
       "concave.points_se.to.1layhid1      -2.440467e+00\n",
       "symmetry_se.to.1layhid1             1.320113e+00\n",
       "fractal_dimension_se.to.1layhid1   -3.066386e-01\n",
       "radius_worst.to.1layhid1           -1.781308e+00\n",
       "texture_worst.to.1layhid1          -1.719174e-01\n",
       "perimeter_worst.to.1layhid1         1.214675e+00\n",
       "area_worst.to.1layhid1              1.895193e+00\n",
       "smoothness_worst.to.1layhid1       -4.304691e-01\n",
       "compactness_worst.to.1layhid1      -2.572694e-01\n",
       "⋮                                  ⋮            \n",
       "1layhid27.to.2layhid5               1.3527838210\n",
       "1layhid28.to.2layhid5               1.9496417349\n",
       "1layhid29.to.2layhid5              -0.1740878757\n",
       "1layhid30.to.2layhid5              -1.4342353538\n",
       "1layhid31.to.2layhid5              -1.0846150093\n",
       "1layhid32.to.2layhid5              -0.7720077560\n",
       "1layhid33.to.2layhid5              -0.4265097047\n",
       "1layhid34.to.2layhid5              -1.2109975554\n",
       "1layhid35.to.2layhid5               0.5977929978\n",
       "1layhid36.to.2layhid5               1.1984208022\n",
       "1layhid37.to.2layhid5              -0.4432986256\n",
       "1layhid38.to.2layhid5              -1.2658084526\n",
       "1layhid39.to.2layhid5              -1.8397791480\n",
       "1layhid40.to.2layhid5               0.5446766251\n",
       "1layhid41.to.2layhid5               0.0315226220\n",
       "1layhid42.to.2layhid5              -2.1303297383\n",
       "1layhid43.to.2layhid5               3.7134972189\n",
       "1layhid44.to.2layhid5               0.0003283772\n",
       "1layhid45.to.2layhid5              -0.8437783447\n",
       "1layhid46.to.2layhid5              -1.2213333704\n",
       "1layhid47.to.2layhid5              -0.4529859936\n",
       "1layhid48.to.2layhid5              -0.7023199313\n",
       "1layhid49.to.2layhid5              -0.8998553724\n",
       "1layhid50.to.2layhid5              -2.3030231905\n",
       "Intercept.to.train_data_y          -0.4765132163\n",
       "2layhid1.to.train_data_y            2.4440238563\n",
       "2layhid2.to.train_data_y            0.8288879272\n",
       "2layhid3.to.train_data_y            1.7460664205\n",
       "2layhid4.to.train_data_y            0.3461362428\n",
       "2layhid5.to.train_data_y           -5.0883287388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn$result.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data.frame(test_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 170 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>actual</th><th scope=col>prediction</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> TRUE</td><td>0.9856103</td></tr>\n",
       "\t<tr><th scope=row>5</th><td> TRUE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>6</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>7</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>8</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>9</th><td> TRUE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>12</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>20</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>25</th><td> TRUE</td><td>0.9797260</td></tr>\n",
       "\t<tr><th scope=row>26</th><td> TRUE</td><td>0.9680141</td></tr>\n",
       "\t<tr><th scope=row>30</th><td> TRUE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>31</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>35</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>41</th><td> TRUE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>51</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>52</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>54</th><td> TRUE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>55</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>56</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>57</th><td> TRUE</td><td>0.9856103</td></tr>\n",
       "\t<tr><th scope=row>60</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>61</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>63</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>64</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>68</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>70</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>71</th><td> TRUE</td><td>0.9818796</td></tr>\n",
       "\t<tr><th scope=row>72</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><th scope=row>459</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>460</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>463</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>469</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>472</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>476</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>483</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>494</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>496</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>499</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>504</th><td> TRUE</td><td>0.9790895</td></tr>\n",
       "\t<tr><th scope=row>515</th><td> TRUE</td><td>0.2115599</td></tr>\n",
       "\t<tr><th scope=row>517</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>519</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>523</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>527</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>530</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>535</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>536</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "\t<tr><th scope=row>539</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>543</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>544</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>546</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>550</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>552</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>555</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>557</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>559</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>560</th><td>FALSE</td><td>0.1871773</td></tr>\n",
       "\t<tr><th scope=row>566</th><td> TRUE</td><td>0.9679749</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 170 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & actual & prediction\\\\\n",
       "  & <lgl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 &  TRUE & 0.9856103\\\\\n",
       "\t5 &  TRUE & 0.1871773\\\\\n",
       "\t6 &  TRUE & 0.9679749\\\\\n",
       "\t7 &  TRUE & 0.9679749\\\\\n",
       "\t8 &  TRUE & 0.9679749\\\\\n",
       "\t9 &  TRUE & 0.1871773\\\\\n",
       "\t12 &  TRUE & 0.9679749\\\\\n",
       "\t20 & FALSE & 0.1871773\\\\\n",
       "\t21 & FALSE & 0.1871773\\\\\n",
       "\t22 & FALSE & 0.1871773\\\\\n",
       "\t25 &  TRUE & 0.9797260\\\\\n",
       "\t26 &  TRUE & 0.9680141\\\\\n",
       "\t30 &  TRUE & 0.1871773\\\\\n",
       "\t31 &  TRUE & 0.9679749\\\\\n",
       "\t35 &  TRUE & 0.9679749\\\\\n",
       "\t41 &  TRUE & 0.1871773\\\\\n",
       "\t51 & FALSE & 0.1871773\\\\\n",
       "\t52 & FALSE & 0.1871773\\\\\n",
       "\t54 &  TRUE & 0.1871773\\\\\n",
       "\t55 &  TRUE & 0.9679749\\\\\n",
       "\t56 & FALSE & 0.1871773\\\\\n",
       "\t57 &  TRUE & 0.9856103\\\\\n",
       "\t60 & FALSE & 0.1871773\\\\\n",
       "\t61 & FALSE & 0.1871773\\\\\n",
       "\t63 &  TRUE & 0.9679749\\\\\n",
       "\t64 & FALSE & 0.1871773\\\\\n",
       "\t68 & FALSE & 0.1871773\\\\\n",
       "\t70 & FALSE & 0.1871773\\\\\n",
       "\t71 &  TRUE & 0.9818796\\\\\n",
       "\t72 & FALSE & 0.1871773\\\\\n",
       "\t⋮ & ⋮ & ⋮\\\\\n",
       "\t459 & FALSE & 0.1871773\\\\\n",
       "\t460 & FALSE & 0.1871773\\\\\n",
       "\t463 & FALSE & 0.1871773\\\\\n",
       "\t469 &  TRUE & 0.9679749\\\\\n",
       "\t472 & FALSE & 0.1871773\\\\\n",
       "\t476 & FALSE & 0.1871773\\\\\n",
       "\t483 & FALSE & 0.1871773\\\\\n",
       "\t494 & FALSE & 0.1871773\\\\\n",
       "\t496 & FALSE & 0.1871773\\\\\n",
       "\t499 &  TRUE & 0.9679749\\\\\n",
       "\t504 &  TRUE & 0.9790895\\\\\n",
       "\t515 &  TRUE & 0.2115599\\\\\n",
       "\t517 &  TRUE & 0.9679749\\\\\n",
       "\t519 & FALSE & 0.1871773\\\\\n",
       "\t523 & FALSE & 0.1871773\\\\\n",
       "\t527 & FALSE & 0.1871773\\\\\n",
       "\t530 & FALSE & 0.1871773\\\\\n",
       "\t535 & FALSE & 0.1871773\\\\\n",
       "\t536 &  TRUE & 0.9679749\\\\\n",
       "\t539 & FALSE & 0.1871773\\\\\n",
       "\t543 & FALSE & 0.1871773\\\\\n",
       "\t544 & FALSE & 0.1871773\\\\\n",
       "\t546 & FALSE & 0.1871773\\\\\n",
       "\t550 & FALSE & 0.1871773\\\\\n",
       "\t552 & FALSE & 0.1871773\\\\\n",
       "\t555 & FALSE & 0.1871773\\\\\n",
       "\t557 & FALSE & 0.1871773\\\\\n",
       "\t559 & FALSE & 0.1871773\\\\\n",
       "\t560 & FALSE & 0.1871773\\\\\n",
       "\t566 &  TRUE & 0.9679749\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 170 × 2\n",
       "\n",
       "| <!--/--> | actual &lt;lgl&gt; | prediction &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 |  TRUE | 0.9856103 |\n",
       "| 5 |  TRUE | 0.1871773 |\n",
       "| 6 |  TRUE | 0.9679749 |\n",
       "| 7 |  TRUE | 0.9679749 |\n",
       "| 8 |  TRUE | 0.9679749 |\n",
       "| 9 |  TRUE | 0.1871773 |\n",
       "| 12 |  TRUE | 0.9679749 |\n",
       "| 20 | FALSE | 0.1871773 |\n",
       "| 21 | FALSE | 0.1871773 |\n",
       "| 22 | FALSE | 0.1871773 |\n",
       "| 25 |  TRUE | 0.9797260 |\n",
       "| 26 |  TRUE | 0.9680141 |\n",
       "| 30 |  TRUE | 0.1871773 |\n",
       "| 31 |  TRUE | 0.9679749 |\n",
       "| 35 |  TRUE | 0.9679749 |\n",
       "| 41 |  TRUE | 0.1871773 |\n",
       "| 51 | FALSE | 0.1871773 |\n",
       "| 52 | FALSE | 0.1871773 |\n",
       "| 54 |  TRUE | 0.1871773 |\n",
       "| 55 |  TRUE | 0.9679749 |\n",
       "| 56 | FALSE | 0.1871773 |\n",
       "| 57 |  TRUE | 0.9856103 |\n",
       "| 60 | FALSE | 0.1871773 |\n",
       "| 61 | FALSE | 0.1871773 |\n",
       "| 63 |  TRUE | 0.9679749 |\n",
       "| 64 | FALSE | 0.1871773 |\n",
       "| 68 | FALSE | 0.1871773 |\n",
       "| 70 | FALSE | 0.1871773 |\n",
       "| 71 |  TRUE | 0.9818796 |\n",
       "| 72 | FALSE | 0.1871773 |\n",
       "| ⋮ | ⋮ | ⋮ |\n",
       "| 459 | FALSE | 0.1871773 |\n",
       "| 460 | FALSE | 0.1871773 |\n",
       "| 463 | FALSE | 0.1871773 |\n",
       "| 469 |  TRUE | 0.9679749 |\n",
       "| 472 | FALSE | 0.1871773 |\n",
       "| 476 | FALSE | 0.1871773 |\n",
       "| 483 | FALSE | 0.1871773 |\n",
       "| 494 | FALSE | 0.1871773 |\n",
       "| 496 | FALSE | 0.1871773 |\n",
       "| 499 |  TRUE | 0.9679749 |\n",
       "| 504 |  TRUE | 0.9790895 |\n",
       "| 515 |  TRUE | 0.2115599 |\n",
       "| 517 |  TRUE | 0.9679749 |\n",
       "| 519 | FALSE | 0.1871773 |\n",
       "| 523 | FALSE | 0.1871773 |\n",
       "| 527 | FALSE | 0.1871773 |\n",
       "| 530 | FALSE | 0.1871773 |\n",
       "| 535 | FALSE | 0.1871773 |\n",
       "| 536 |  TRUE | 0.9679749 |\n",
       "| 539 | FALSE | 0.1871773 |\n",
       "| 543 | FALSE | 0.1871773 |\n",
       "| 544 | FALSE | 0.1871773 |\n",
       "| 546 | FALSE | 0.1871773 |\n",
       "| 550 | FALSE | 0.1871773 |\n",
       "| 552 | FALSE | 0.1871773 |\n",
       "| 555 | FALSE | 0.1871773 |\n",
       "| 557 | FALSE | 0.1871773 |\n",
       "| 559 | FALSE | 0.1871773 |\n",
       "| 560 | FALSE | 0.1871773 |\n",
       "| 566 |  TRUE | 0.9679749 |\n",
       "\n"
      ],
      "text/plain": [
       "    actual prediction\n",
       "1    TRUE  0.9856103 \n",
       "5    TRUE  0.1871773 \n",
       "6    TRUE  0.9679749 \n",
       "7    TRUE  0.9679749 \n",
       "8    TRUE  0.9679749 \n",
       "9    TRUE  0.1871773 \n",
       "12   TRUE  0.9679749 \n",
       "20  FALSE  0.1871773 \n",
       "21  FALSE  0.1871773 \n",
       "22  FALSE  0.1871773 \n",
       "25   TRUE  0.9797260 \n",
       "26   TRUE  0.9680141 \n",
       "30   TRUE  0.1871773 \n",
       "31   TRUE  0.9679749 \n",
       "35   TRUE  0.9679749 \n",
       "41   TRUE  0.1871773 \n",
       "51  FALSE  0.1871773 \n",
       "52  FALSE  0.1871773 \n",
       "54   TRUE  0.1871773 \n",
       "55   TRUE  0.9679749 \n",
       "56  FALSE  0.1871773 \n",
       "57   TRUE  0.9856103 \n",
       "60  FALSE  0.1871773 \n",
       "61  FALSE  0.1871773 \n",
       "63   TRUE  0.9679749 \n",
       "64  FALSE  0.1871773 \n",
       "68  FALSE  0.1871773 \n",
       "70  FALSE  0.1871773 \n",
       "71   TRUE  0.9818796 \n",
       "72  FALSE  0.1871773 \n",
       "⋮   ⋮      ⋮         \n",
       "459 FALSE  0.1871773 \n",
       "460 FALSE  0.1871773 \n",
       "463 FALSE  0.1871773 \n",
       "469  TRUE  0.9679749 \n",
       "472 FALSE  0.1871773 \n",
       "476 FALSE  0.1871773 \n",
       "483 FALSE  0.1871773 \n",
       "494 FALSE  0.1871773 \n",
       "496 FALSE  0.1871773 \n",
       "499  TRUE  0.9679749 \n",
       "504  TRUE  0.9790895 \n",
       "515  TRUE  0.2115599 \n",
       "517  TRUE  0.9679749 \n",
       "519 FALSE  0.1871773 \n",
       "523 FALSE  0.1871773 \n",
       "527 FALSE  0.1871773 \n",
       "530 FALSE  0.1871773 \n",
       "535 FALSE  0.1871773 \n",
       "536  TRUE  0.9679749 \n",
       "539 FALSE  0.1871773 \n",
       "543 FALSE  0.1871773 \n",
       "544 FALSE  0.1871773 \n",
       "546 FALSE  0.1871773 \n",
       "550 FALSE  0.1871773 \n",
       "552 FALSE  0.1871773 \n",
       "555 FALSE  0.1871773 \n",
       "557 FALSE  0.1871773 \n",
       "559 FALSE  0.1871773 \n",
       "560 FALSE  0.1871773 \n",
       "566  TRUE  0.9679749 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.results=compute(nn,test)\n",
    "results <- data.frame(actual = test_data_y, prediction = nn.results$net.result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      prediction\n",
       "actual   0   1\n",
       "     0 107   0\n",
       "     1  22  41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roundedresults<-sapply(results,round,digits=0)\n",
    "roundedresultsdf=data.frame(roundedresults)\n",
    "attach(roundedresultsdf)\n",
    "table(actual,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>truth</th><th scope=col>input1</th><th scope=col>input2</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0.2</td><td>0.9</td></tr>\n",
       "\t<tr><td>0</td><td>0.1</td><td>0.2</td></tr>\n",
       "\t<tr><td>0</td><td>0.3</td><td>0.2</td></tr>\n",
       "\t<tr><td>1</td><td>0.2</td><td>0.5</td></tr>\n",
       "\t<tr><td>1</td><td>0.8</td><td>0.5</td></tr>\n",
       "\t<tr><td>1</td><td>0.3</td><td>0.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " truth & input1 & input2\\\\\n",
       " <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 0.2 & 0.9\\\\\n",
       "\t 0 & 0.1 & 0.2\\\\\n",
       "\t 0 & 0.3 & 0.2\\\\\n",
       "\t 1 & 0.2 & 0.5\\\\\n",
       "\t 1 & 0.8 & 0.5\\\\\n",
       "\t 1 & 0.3 & 0.8\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| truth &lt;dbl&gt; | input1 &lt;dbl&gt; | input2 &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 0.2 | 0.9 |\n",
       "| 0 | 0.1 | 0.2 |\n",
       "| 0 | 0.3 | 0.2 |\n",
       "| 1 | 0.2 | 0.5 |\n",
       "| 1 | 0.8 | 0.5 |\n",
       "| 1 | 0.3 | 0.8 |\n",
       "\n"
      ],
      "text/plain": [
       "  truth input1 input2\n",
       "1 1     0.2    0.9   \n",
       "2 0     0.1    0.2   \n",
       "3 0     0.3    0.2   \n",
       "4 1     0.2    0.5   \n",
       "5 1     0.8    0.5   \n",
       "6 1     0.3    0.8   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input1=c(20,10,30,20,80,30)/100\n",
    "input2=c(90,20,20,50,50,80)/100\n",
    "\n",
    "truth=c(1,0,0,1,1,1)\n",
    "# Here, you will combine multiple columns or features into a single set of data\n",
    "df=data.frame(truth,input1,input2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit neural network\n",
    "nn=neuralnet(truth ~ input1+input2, \n",
    "             data=df, \n",
    "             hidden=c(3), \n",
    "             linear.output=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$call\n",
      "neuralnet(formula = truth ~ input1 + input2, data = df, hidden = c(3), \n",
      "    linear.output = FALSE)\n",
      "\n",
      "$response\n",
      "  truth\n",
      "1     1\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "\n",
      "$covariate\n",
      "     input1 input2\n",
      "[1,]    0.2    0.9\n",
      "[2,]    0.1    0.2\n",
      "[3,]    0.3    0.2\n",
      "[4,]    0.2    0.5\n",
      "[5,]    0.8    0.5\n",
      "[6,]    0.3    0.8\n",
      "\n",
      "$model.list\n",
      "$model.list$response\n",
      "[1] \"truth\"\n",
      "\n",
      "$model.list$variables\n",
      "[1] \"input1\" \"input2\"\n",
      "\n",
      "\n",
      "$err.fct\n",
      "function (x, y) \n",
      "{\n",
      "    1/2 * (y - x)^2\n",
      "}\n",
      "<bytecode: 0x55a0593b62f8>\n",
      "<environment: 0x55a064649318>\n",
      "attr(,\"type\")\n",
      "[1] \"sse\"\n",
      "\n",
      "$act.fct\n",
      "function (x) \n",
      "{\n",
      "    1/(1 + exp(-x))\n",
      "}\n",
      "<bytecode: 0x55a0593c2918>\n",
      "<environment: 0x55a0646497b0>\n",
      "attr(,\"type\")\n",
      "[1] \"logistic\"\n",
      "\n",
      "$linear.output\n",
      "[1] FALSE\n",
      "\n",
      "$data\n",
      "  truth input1 input2\n",
      "1     1    0.2    0.9\n",
      "2     0    0.1    0.2\n",
      "3     0    0.3    0.2\n",
      "4     1    0.2    0.5\n",
      "5     1    0.8    0.5\n",
      "6     1    0.3    0.8\n",
      "\n",
      "$exclude\n",
      "NULL\n",
      "\n",
      "$net.result\n",
      "$net.result[[1]]\n",
      "          [,1]\n",
      "[1,] 0.9906741\n",
      "[2,] 0.0857410\n",
      "[3,] 0.1422116\n",
      "[4,] 0.8571457\n",
      "[5,] 0.9614236\n",
      "[6,] 0.9875551\n",
      "\n",
      "\n",
      "$weights\n",
      "$weights[[1]]\n",
      "$weights[[1]][[1]]\n",
      "          [,1]      [,2]       [,3]\n",
      "[1,] -2.110310  1.830563 -1.6011840\n",
      "[2,]  1.294161 -1.438388  0.4573975\n",
      "[3,]  5.264905 -4.470830  4.6653859\n",
      "\n",
      "$weights[[1]][[2]]\n",
      "          [,1]\n",
      "[1,] -1.128667\n",
      "[2,]  3.579079\n",
      "[3,] -4.774114\n",
      "[4,]  2.961159\n",
      "\n",
      "\n",
      "\n",
      "$generalized.weights\n",
      "$generalized.weights[[1]]\n",
      "          [,1]      [,2]\n",
      "[1,] 0.8016186  3.286176\n",
      "[2,] 2.7214540 11.544775\n",
      "[3,] 2.9651354 12.451964\n",
      "[4,] 2.8114140 11.733004\n",
      "[5,] 1.9080805  8.371860\n",
      "[6,] 1.0704026  4.457739\n",
      "\n",
      "\n",
      "$startweights\n",
      "$startweights[[1]]\n",
      "$startweights[[1]][[1]]\n",
      "          [,1]      [,2]       [,3]\n",
      "[1,] 0.5023422 0.8112535 -1.2775016\n",
      "[2,] 1.0998450 1.4182438 -0.8613732\n",
      "[3,] 1.8865048 0.4391699  1.8869859\n",
      "\n",
      "$startweights[[1]][[2]]\n",
      "           [,1]\n",
      "[1,]  0.7040873\n",
      "[2,] -0.6677205\n",
      "[3,] -0.7057143\n",
      "[4,] -0.9207186\n",
      "\n",
      "\n",
      "\n",
      "$result.matrix\n",
      "                              [,1]\n",
      "error                  0.024856499\n",
      "reached.threshold      0.009340404\n",
      "steps                 53.000000000\n",
      "Intercept.to.1layhid1 -2.110310372\n",
      "input1.to.1layhid1     1.294160947\n",
      "input2.to.1layhid1     5.264904830\n",
      "Intercept.to.1layhid2  1.830562941\n",
      "input1.to.1layhid2    -1.438388372\n",
      "input2.to.1layhid2    -4.470830098\n",
      "Intercept.to.1layhid3 -1.601184038\n",
      "input1.to.1layhid3     0.457397515\n",
      "input2.to.1layhid3     4.665385875\n",
      "Intercept.to.truth    -1.128666927\n",
      "1layhid1.to.truth      3.579079475\n",
      "1layhid2.to.truth     -4.774114342\n",
      "1layhid3.to.truth      2.961158540\n",
      "\n",
      "attr(,\"class\")\n",
      "[1] \"nn\"\n"
     ]
    }
   ],
   "source": [
    "print(nn)\n",
    "plot(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test set\n",
    "input1=c(30,30,85)/100\n",
    "input2=c(85,10,40)/100\n",
    "\n",
    "test=data.frame(input1,input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 3 × 1 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.98981090</td></tr>\n",
       "\t<tr><td>0.05001758</td></tr>\n",
       "\t<tr><td>0.91527967</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 3 × 1 of type dbl\n",
       "\\begin{tabular}{l}\n",
       "\t 0.98981090\\\\\n",
       "\t 0.05001758\\\\\n",
       "\t 0.91527967\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 3 × 1 of type dbl\n",
       "\n",
       "| 0.98981090 |\n",
       "| 0.05001758 |\n",
       "| 0.91527967 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      \n",
       "[1,] 0.98981090\n",
       "[2,] 0.05001758\n",
       "[3,] 0.91527967"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Prediction using neural network\n",
    "Predict=compute(nn,test)\n",
    "Predict$net.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA=0.1 # learning parameter.... step_size\n",
    "\n",
    "nodes=c(2,3,1) # 5 inputs, 2 hidden layers, with 7 and 10 nodes , 1 output\n",
    "\n",
    "nlayers=length(nodes) -1 # 3 sets of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<caption>A matrix: 3 × 2 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td> 0.4148060</td><td>0.33044763</td></tr>\n",
       "\t<tr><td> 0.4370754</td><td>0.14174552</td></tr>\n",
       "\t<tr><td>-0.2138605</td><td>0.01909595</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<caption>A matrix: 1 × 3 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.2365883</td><td>-0.3653334</td><td>0.1569923</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item A matrix: 3 × 2 of type dbl\n",
       "\\begin{tabular}{ll}\n",
       "\t  0.4148060 & 0.33044763\\\\\n",
       "\t  0.4370754 & 0.14174552\\\\\n",
       "\t -0.2138605 & 0.01909595\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item A matrix: 1 × 3 of type dbl\n",
       "\\begin{tabular}{lll}\n",
       "\t 0.2365883 & -0.3653334 & 0.1569923\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "A matrix: 3 × 2 of type dbl\n",
       "\n",
       "|  0.4148060 | 0.33044763 |\n",
       "|  0.4370754 | 0.14174552 |\n",
       "| -0.2138605 | 0.01909595 |\n",
       "\n",
       "\n",
       "2. \n",
       "A matrix: 1 × 3 of type dbl\n",
       "\n",
       "| 0.2365883 | -0.3653334 | 0.1569923 |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "           [,1]       [,2]\n",
       "[1,]  0.4148060 0.33044763\n",
       "[2,]  0.4370754 0.14174552\n",
       "[3,] -0.2138605 0.01909595\n",
       "\n",
       "[[2]]\n",
       "          [,1]       [,2]      [,3]\n",
       "[1,] 0.2365883 -0.3653334 0.1569923\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net=list() # set up empty list\n",
    "# net[[ j ]] holds weight matrix feeding nodes of layer j+1 from nodes in layer j\n",
    "\n",
    "set.seed(42)\n",
    "# make weights and fill with random numbers\n",
    "for(j in 1:nlayers) net[[ j ]] <- (matrix(runif(nodes[ j ]*nodes[ j +1 ]),nodes[j+1],nodes[j]) - 0.5)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [,1]     [,2]      [,3]      [,4]      [,5]      [,6]\n",
      "[1,] 0.5039473 0.503303 0.5021143 0.5032493 0.4996885 0.5031735\n"
     ]
    }
   ],
   "source": [
    "netsays <- function(x) { # Returns net output for some input vector x\n",
    "  for(j in 1:nlayers) x <- 1/(1+exp(-net[[ j ]] %*% x))\n",
    "  return(x)\n",
    "}\n",
    "\n",
    "print(netsays(t(df[,-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our ANN's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 1 × 3 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.5032585</td><td>0.5019324</td><td>0.4992267</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1 × 3 of type dbl\n",
       "\\begin{tabular}{lll}\n",
       "\t 0.5032585 & 0.5019324 & 0.4992267\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1 × 3 of type dbl\n",
       "\n",
       "| 0.5032585 | 0.5019324 | 0.4992267 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      [,2]      [,3]     \n",
       "[1,] 0.5032585 0.5019324 0.4992267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netsays(t(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop <- function(layer,n1, n2, factor){ # recursive function used for back-propagation\n",
    "  if(layer>1) for(n in 1:nodes[layer-1])\n",
    "    backprop(layer-1,n2,n,factor*net[[layer]][n1,n2]*r[[layer]][n2]*(1-r[[layer]][n2]))\n",
    "  net[[layer]][n1,n2] <<- net[[layer]][n1,n2] - ALPHA*factor*r[[layer]][n2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlearns <- function(x,truth) { # like netsays but changes weights\n",
    "  r <<- list() # to contain the outputs of all nodes in all layers\n",
    "  r[[1]] <<- x # the input layer\n",
    "  for(layer in 1:nlayers) r[[layer+1]] <<- as.vector(1/(1+exp(-net[[layer]] %*% r[[layer]])))\n",
    "  u <- r[[nlayers+1]] # final answer, for convenience\n",
    "  for(n in 1:nodes[nlayers]) backprop(nlayers,1,n,(u-truth)*u*(1-u))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:5000){\n",
    "    for(i in 1:6) netlearns(t(df[i,-1]), df[i,1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 1 × 3 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.9779823</td><td>0.02916231</td><td>0.7521905</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1 × 3 of type dbl\n",
       "\\begin{tabular}{lll}\n",
       "\t 0.9779823 & 0.02916231 & 0.7521905\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1 × 3 of type dbl\n",
       "\n",
       "| 0.9779823 | 0.02916231 | 0.7521905 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      [,2]       [,3]     \n",
       "[1,] 0.9779823 0.02916231 0.7521905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netsays(t(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample <- read.table(\"Sample1.txt\",header=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>-0.059978730</td><td>3.881889</td><td>1.0607440</td><td>4.022852</td><td>-0.05597012</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td> 0.881978000</td><td>2.055923</td><td>3.1585140</td><td>1.972982</td><td> 1.19097300</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0</td><td> 0.077789470</td><td>3.950015</td><td>0.9496442</td><td>3.976893</td><td> 0.04745127</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1</td><td> 0.975983300</td><td>2.032230</td><td>2.9900490</td><td>2.017683</td><td> 1.06281300</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>-0.001502924</td><td>3.862673</td><td>0.8942838</td><td>4.020337</td><td>-0.02683437</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0</td><td> 0.073092370</td><td>3.982063</td><td>1.0439070</td><td>3.860677</td><td>-0.13946140</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & V1 & V2 & V3 & V4 & V5 & V6\\\\\n",
       "  & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0 & -0.059978730 & 3.881889 & 1.0607440 & 4.022852 & -0.05597012\\\\\n",
       "\t2 & 1 &  0.881978000 & 2.055923 & 3.1585140 & 1.972982 &  1.19097300\\\\\n",
       "\t3 & 0 &  0.077789470 & 3.950015 & 0.9496442 & 3.976893 &  0.04745127\\\\\n",
       "\t4 & 1 &  0.975983300 & 2.032230 & 2.9900490 & 2.017683 &  1.06281300\\\\\n",
       "\t5 & 0 & -0.001502924 & 3.862673 & 0.8942838 & 4.020337 & -0.02683437\\\\\n",
       "\t6 & 0 &  0.073092370 & 3.982063 & 1.0439070 & 3.860677 & -0.13946140\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | V1 &lt;int&gt; | V2 &lt;dbl&gt; | V3 &lt;dbl&gt; | V4 &lt;dbl&gt; | V5 &lt;dbl&gt; | V6 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | 0 | -0.059978730 | 3.881889 | 1.0607440 | 4.022852 | -0.05597012 |\n",
       "| 2 | 1 |  0.881978000 | 2.055923 | 3.1585140 | 1.972982 |  1.19097300 |\n",
       "| 3 | 0 |  0.077789470 | 3.950015 | 0.9496442 | 3.976893 |  0.04745127 |\n",
       "| 4 | 1 |  0.975983300 | 2.032230 | 2.9900490 | 2.017683 |  1.06281300 |\n",
       "| 5 | 0 | -0.001502924 | 3.862673 | 0.8942838 | 4.020337 | -0.02683437 |\n",
       "| 6 | 0 |  0.073092370 | 3.982063 | 1.0439070 | 3.860677 | -0.13946140 |\n",
       "\n"
      ],
      "text/plain": [
       "  V1 V2           V3       V4        V5       V6         \n",
       "1 0  -0.059978730 3.881889 1.0607440 4.022852 -0.05597012\n",
       "2 1   0.881978000 2.055923 3.1585140 1.972982  1.19097300\n",
       "3 0   0.077789470 3.950015 0.9496442 3.976893  0.04745127\n",
       "4 1   0.975983300 2.032230 2.9900490 2.017683  1.06281300\n",
       "5 0  -0.001502924 3.862673 0.8942838 4.020337 -0.02683437\n",
       "6 0   0.073092370 3.982063 1.0439070 3.860677 -0.13946140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hidden: 8, 5    thresh: 0.01    rep: 1/1    steps:      22\terror: 0.00674\ttime: 0.02 secs\n"
     ]
    }
   ],
   "source": [
    "# fit neural network\n",
    "nn=neuralnet(V1 ~ ., \n",
    "             data=sample, \n",
    "             lifesign='full',\n",
    "             hidden=c(8,5), \n",
    "             linear.output=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict=compute(nn,sample)\n",
    "# Predict$net.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>actual</th><th scope=col>prediction</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>0.003782127</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td>0.996466719</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0</td><td>0.003788244</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1</td><td>0.996464589</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>0.003783586</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0</td><td>0.003783776</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & actual & prediction\\\\\n",
       "  & <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 0.003782127\\\\\n",
       "\t2 & 1 & 0.996466719\\\\\n",
       "\t3 & 0 & 0.003788244\\\\\n",
       "\t4 & 1 & 0.996464589\\\\\n",
       "\t5 & 0 & 0.003783586\\\\\n",
       "\t6 & 0 & 0.003783776\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | actual &lt;int&gt; | prediction &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 0 | 0.003782127 |\n",
       "| 2 | 1 | 0.996466719 |\n",
       "| 3 | 0 | 0.003788244 |\n",
       "| 4 | 1 | 0.996464589 |\n",
       "| 5 | 0 | 0.003783586 |\n",
       "| 6 | 0 | 0.003783776 |\n",
       "\n"
      ],
      "text/plain": [
       "  actual prediction \n",
       "1 0      0.003782127\n",
       "2 1      0.996466719\n",
       "3 0      0.003788244\n",
       "4 1      0.996464589\n",
       "5 0      0.003783586\n",
       "6 0      0.003783776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results <- data.frame(actual = sample$V1, \n",
    "                      prediction = Predict$net.result)\n",
    "head(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from roundedresultsdf (pos = 3):\n",
      "\n",
      "    actual, prediction\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      prediction\n",
       "actual   0   1\n",
       "     0 534   0\n",
       "     1   0 466"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roundedresults<-sapply(results,round,digits=0)\n",
    "roundedresultsdf=data.frame(roundedresults)\n",
    "attach(roundedresultsdf)\n",
    "table(actual,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test <- read.table(\"Sample2.txt\",header=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td> 1.5870520</td><td> 4.715568</td><td>-0.8595715</td><td> 1.5040090</td><td> 2.1454170</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td> 2.5206200</td><td> 2.682234</td><td> 3.9096930</td><td> 0.2611399</td><td> 0.3924642</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1</td><td>-0.5450664</td><td>-1.449915</td><td>-0.2813677</td><td> 4.0579420</td><td> 0.9299015</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0</td><td>-1.0479510</td><td> 4.223808</td><td> 3.0683020</td><td> 9.6731960</td><td> 3.9158380</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1</td><td>-2.8632640</td><td> 1.250906</td><td> 0.2937350</td><td>-0.2080808</td><td>-0.6673748</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1</td><td>-0.2963963</td><td> 2.988054</td><td> 1.4497160</td><td> 2.3261870</td><td>-0.5594592</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & V1 & V2 & V3 & V4 & V5 & V6\\\\\n",
       "  & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0 &  1.5870520 &  4.715568 & -0.8595715 &  1.5040090 &  2.1454170\\\\\n",
       "\t2 & 1 &  2.5206200 &  2.682234 &  3.9096930 &  0.2611399 &  0.3924642\\\\\n",
       "\t3 & 1 & -0.5450664 & -1.449915 & -0.2813677 &  4.0579420 &  0.9299015\\\\\n",
       "\t4 & 0 & -1.0479510 &  4.223808 &  3.0683020 &  9.6731960 &  3.9158380\\\\\n",
       "\t5 & 1 & -2.8632640 &  1.250906 &  0.2937350 & -0.2080808 & -0.6673748\\\\\n",
       "\t6 & 1 & -0.2963963 &  2.988054 &  1.4497160 &  2.3261870 & -0.5594592\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | V1 &lt;int&gt; | V2 &lt;dbl&gt; | V3 &lt;dbl&gt; | V4 &lt;dbl&gt; | V5 &lt;dbl&gt; | V6 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | 0 |  1.5870520 |  4.715568 | -0.8595715 |  1.5040090 |  2.1454170 |\n",
       "| 2 | 1 |  2.5206200 |  2.682234 |  3.9096930 |  0.2611399 |  0.3924642 |\n",
       "| 3 | 1 | -0.5450664 | -1.449915 | -0.2813677 |  4.0579420 |  0.9299015 |\n",
       "| 4 | 0 | -1.0479510 |  4.223808 |  3.0683020 |  9.6731960 |  3.9158380 |\n",
       "| 5 | 1 | -2.8632640 |  1.250906 |  0.2937350 | -0.2080808 | -0.6673748 |\n",
       "| 6 | 1 | -0.2963963 |  2.988054 |  1.4497160 |  2.3261870 | -0.5594592 |\n",
       "\n"
      ],
      "text/plain": [
       "  V1 V2         V3        V4         V5         V6        \n",
       "1 0   1.5870520  4.715568 -0.8595715  1.5040090  2.1454170\n",
       "2 1   2.5206200  2.682234  3.9096930  0.2611399  0.3924642\n",
       "3 1  -0.5450664 -1.449915 -0.2813677  4.0579420  0.9299015\n",
       "4 0  -1.0479510  4.223808  3.0683020  9.6731960  3.9158380\n",
       "5 1  -2.8632640  1.250906  0.2937350 -0.2080808 -0.6673748\n",
       "6 1  -0.2963963  2.988054  1.4497160  2.3261870 -0.5594592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictTest=compute(nn,sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>actual</th><th scope=col>prediction</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>0.996171792</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td>0.996466768</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1</td><td>0.189931704</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0</td><td>0.956760944</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1</td><td>0.003802225</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1</td><td>0.003807829</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & actual & prediction\\\\\n",
       "  & <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 0.996171792\\\\\n",
       "\t2 & 1 & 0.996466768\\\\\n",
       "\t3 & 1 & 0.189931704\\\\\n",
       "\t4 & 0 & 0.956760944\\\\\n",
       "\t5 & 1 & 0.003802225\\\\\n",
       "\t6 & 1 & 0.003807829\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | actual &lt;int&gt; | prediction &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 0 | 0.996171792 |\n",
       "| 2 | 1 | 0.996466768 |\n",
       "| 3 | 1 | 0.189931704 |\n",
       "| 4 | 0 | 0.956760944 |\n",
       "| 5 | 1 | 0.003802225 |\n",
       "| 6 | 1 | 0.003807829 |\n",
       "\n"
      ],
      "text/plain": [
       "  actual prediction \n",
       "1 0      0.996171792\n",
       "2 1      0.996466768\n",
       "3 1      0.189931704\n",
       "4 0      0.956760944\n",
       "5 1      0.003802225\n",
       "6 1      0.003807829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test <- data.frame(actual = sample_test$V1, \n",
    "                           prediction = PredictTest$net.result)\n",
    "head(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from roundedresultsdf (pos = 3):\n",
      "\n",
      "    actual, prediction\n",
      "\n",
      "The following objects are masked from roundedresultsdf (pos = 4):\n",
      "\n",
      "    actual, prediction\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      prediction\n",
       "actual   0   1\n",
       "     0 354 142\n",
       "     1 143 361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roundedresults<-sapply(results_test,round,digits=0)\n",
    "roundedresultsdf=data.frame(roundedresults)\n",
    "attach(roundedresultsdf)\n",
    "table(actual,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
